import json

# Define the notebook structure
notebook = {
    "cells": [],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        },
        "colab": {
            "provenance": [],
            "gpuType": "A100"
        },
        "accelerator": "GPU"
    },
    "nbformat": 4,
    "nbformat_minor": 0
}

def add_markdown(source_string):
    lines = [line + "
" for line in source_string.splitlines()]
    if lines: lines[-1] = lines[-1].rstrip("
")
    notebook["cells"].append({
        "cell_type": "markdown",
        "metadata": {
            "id": "2KtKer9gg7Il"
        },
        "source": lines
    })

def add_code(source_string, execution_count=None, outputs=None):
    if outputs is None:
        outputs = []
    lines = [line + "
" for line in source_string.splitlines()]
    if lines: lines[-1] = lines[-1].rstrip("
")
    notebook["cells"].append({
        "cell_type": "code",
        "execution_count": execution_count,
        "metadata": {
            "id": "IIfZsdIYg7Iq" if "Environment" in source_string else None
        },
        "outputs": outputs,
        "source": lines
    })

# --- Content ---

# Cell 0 (Markdown)
md_source_0 = '# Stochastic Higher-Order Swendsen-Wang vs WalkSAT\n\nThis notebook compares our **Stochastic Cluster Monte Carlo** algorithm against the industry standard for Random SAT: **WalkSAT**.\n\n## The Contenders\n1.  **Stochastic Swendsen-Wang (Ours)**:\n    *   Physics-based (Cluster Dynamics).\n    *   Uses geometric frustration and percolation.\n    *   **New**: Uses **Exact Hamiltonian Cluster Updates** (Exact Energy Delta) for decision.\n    *   **Schedule**: Logarithmic annealing (dense near $\\omega_{max}$).\n    *   Runs on GPU (Massively Parallel).\n2.  **WalkSAT (Reference)**:\n    *   Stochastic Local Search.\n    *   Greedy + Noise heuristic.\n    *   Runs on CPU (Sequential, fast flips).\n3.  **Complete Swendsen-Wang**:\n    *   Applies percolation to ALL clauses (not just SAT ones).\n    *   Uniform percolation logic.'
add_markdown(md_source_0)

# Cell 1 (Code)
code_source_1 = '# @title 1. Environment & GPU Setup\nimport sys\nimport os\nimport subprocess\nimport time\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport requests\nimport tarfile\nimport io\nimport gzip\nimport random\n\n# Ensure CuPy is available\ntry:\n    import cupy as cp\n    import cupyx.scipy.sparse as cpx\n    import cupyx.scipy.sparse.csgraph as cpx_graph\n    print(f"GPU Detected: {cp.cuda.runtime.getDeviceCount()} device(s)")\nexcept ImportError:\n    print("Installing CuPy...")\n    subprocess.check_call([sys.executable, \'-m\', \'pip\', \'install\', \'cupy-cuda12x\'])\n    import cupy as cp\n    import cupyx.scipy.sparse as cpx\n    import cupyx.scipy.sparse.csgraph as cpx_graph\n\nplt.style.use(\'dark_background\')\nprint("Environment Ready.")'
add_code(code_source_1, execution_count=None, outputs=[{"output_type": "stream", "name": "stdout", "text": ["GPU Detected: 1 device(s)\n", "Environment Ready.\n"]}])

# Cell 2 (Code)
code_source_2 = '# @title 2. Data Generators (Random & SATLIB)\n\ndef generate_random_3sat(N, alpha, seed=None):\n    if seed is not None: np.random.seed(seed)\n    M = int(N * alpha)\n    vars = np.random.randint(1, N + 1, size=(M, 3))\n    signs = np.random.choice([-1, 1], size=(M, 3))\n    return vars * signs, N'
add_code(code_source_2, execution_count=None, outputs=[])

# Cell 3 (Code)
code_source_3 = '# @title 3. The Solver: `SwendsenWangErdosRenyiGPU`\n\nclass SwendsenWangErdosRenyiGPU:\n    def __init__(self, clauses_np, N, beta_scale=15.0, steps_flips=None, a=1.0, dynamics="Metropolis"):\n        self.N = N\n        self.M = len(clauses_np)\n        self.clauses = cp.array(clauses_np)\n        self.beta_scale = beta_scale\n        self.a = a # ErdÅ‘s-RÃ©nyi parameter\n        self.dynamics = dynamics\n        \n        if steps_flips is None:\n            self.steps_flips = 2 * N\n        else:\n            self.steps_flips = steps_flips\n\n        # Literals: Convert 1-based DIMACS to 0-based indices\n        self.lits_idx = cp.ascontiguousarray(cp.abs(self.clauses).astype(cp.int32) - 1)\n        self.lits_sign = cp.ascontiguousarray(cp.sign(self.clauses).astype(cp.int8))\n\n        # Interactions (Triangles)\n        s = self.lits_sign\n        j01 = cp.where(s[:, 0] == s[:, 1], -1, 1)\n        j12 = cp.where(s[:, 1] == s[:, 2], -1, 1)\n        j20 = cp.where(s[:, 2] == s[:, 0], -1, 1)\n        self.J_tri = cp.stack([j01, j12, j20], axis=1).astype(cp.int8)\n\n        # State (Size N, 0-based)\n        self.sigma = cp.random.choice(cp.array([-1, 1], dtype=cp.int8), size=N)\n        \n        self.best_sigma = self.sigma.copy()\n        self.min_energy = 1.0\n\n        # Kernel\n        self.kernel = cp.RawKernel(metropolis_kernel_code, \'run_metropolis_dynamics\', options=(\'-std=c++17\',))\n\n    def energy_check(self):\n        spins = self.sigma[self.lits_idx]\n        is_lit_sat = (spins == self.lits_sign)\n        is_clause_sat = cp.any(is_lit_sat, axis=1)\n        return 1.0 - cp.mean(is_clause_sat)\n\n\n    def _run_dynamics(self, labels, n_comps, omega):\n        # Map Clusters to Clauses\n        lit_clusters = labels[self.lits_idx] # (M, 3)\n        \n        # Valid Clusters: All unique positive labels\n        valid_clusters = cp.unique(labels).astype(cp.int32)\n        valid_clusters = valid_clusters[valid_clusters >= 0]\n        num_valid = len(valid_clusters)\n        \n        if num_valid == 0: return\n\n        # 1. CSR: Cluster -> Vars\n        # Filter out inactive vars (label -1)\n        valid_mask_v = (labels >= 0)\n        active_vars = cp.where(valid_mask_v)[0]\n        active_labels = labels[valid_mask_v]\n        \n        if len(active_vars) == 0: return\n\n        data_v = cp.ones(len(active_vars), dtype=cp.bool_)\n        cluster_to_vars = cpx.coo_matrix(\n            (data_v, (active_labels, active_vars)),\n            shape=(n_comps, self.N)\n        ).tocsr()\n\n        # 2. CSR: Cluster -> Clauses\n        flat_clusters = lit_clusters.flatten()\n        flat_clauses = cp.repeat(cp.arange(self.M), 3)\n        \n        # Filter out pairs where cluster is -1\n        mask_c = (flat_clusters >= 0)\n        flat_clusters = flat_clusters[mask_c]\n        flat_clauses = flat_clauses[mask_c]\n        \n        if len(flat_clusters) == 0: return\n\n        combined_keys = flat_clusters.astype(cp.int64) * self.M + flat_clauses.astype(cp.int64)\n        unique_keys = cp.unique(combined_keys)\n        \n        u_clusters = (unique_keys // self.M).astype(cp.int32)\n        u_clauses = (unique_keys % self.M).astype(cp.int32)\n        data_c = cp.ones(len(u_clusters), dtype=cp.bool_)\n        \n        cluster_to_clauses = cpx.coo_matrix(\n            (data_c, (u_clusters, u_clauses)),\n            shape=(n_comps, self.M)\n        ).tocsr()\n\n        c2c_indptr = cluster_to_clauses.indptr.astype(cp.int32)\n        c2c_indices = cluster_to_clauses.indices.astype(cp.int32)\n        c2v_indptr = cluster_to_vars.indptr.astype(cp.int32)\n        c2v_indices = cluster_to_vars.indices.astype(cp.int32)\n        lit_clusters_ptr = cp.ascontiguousarray(lit_clusters.astype(cp.int32))\n        \n        seed = int(time.time() * 1000) % 1000000007\n\n        self.kernel(\n            (1,), (256,),\n            (\n                self.sigma, c2c_indptr, c2c_indices, c2v_indptr, c2v_indices,\n                self.lits_idx, self.lits_sign, lit_clusters_ptr, valid_clusters,\n                cp.int32(num_valid), cp.int32(self.steps_flips),\n                cp.float32(omega), cp.float32(self.beta_scale), cp.uint64(seed)\n            )\n        )\n\n\n    def step(self, omega, verbose=False):\n        # --- PHASE 1: Standard Cluster Dynamics ---\n        c_spins = self.sigma[self.lits_idx]\n        \n        s0, s1, s2 = c_spins[:, 0], c_spins[:, 1], c_spins[:, 2]\n        sat0 = (s0 * s1 * self.J_tri[:, 0] == 1)\n        sat1 = (s1 * s2 * self.J_tri[:, 1] == 1)\n        sat2 = (s2 * s0 * self.J_tri[:, 2] == 1)\n        sat_mask = cp.stack([sat0, sat1, sat2], axis=1)\n        num_sat_tri = cp.sum(sat_mask, axis=1)\n        is_low_energy = (num_sat_tri == 2)\n\n        P = 1.0 - cp.exp(-omega)\n        rand_vals = cp.random.random(self.M, dtype=cp.float32)\n        src_nodes, dst_nodes = [], []\n\n        mask_T = is_low_energy & (rand_vals < P)\n        if cp.any(mask_T):\n            idx_T = cp.where(mask_T)[0]\n            r_vals_T = rand_vals[idx_T]\n            sub_sat = sat_mask[idx_T]\n            idx_1st = cp.argmax(sub_sat, axis=1)\n            idx_sum = cp.sum(sub_sat * cp.array([0, 1, 2], dtype=cp.int8), axis=1)\n            idx_2nd = idx_sum - idx_1st\n            pick_first = (r_vals_T < (P / 2.0))\n            chosen = cp.where(pick_first, idx_1st, idx_2nd)\n            lits = self.lits_idx[idx_T]\n            l0, l1, l2 = lits[:,0], lits[:,1], lits[:,2]\n            s_e = cp.where(chosen==0, l0, cp.where(chosen==1, l1, l2))\n            d_e = cp.where(chosen==0, l1, cp.where(chosen==1, l2, l0))\n            src_nodes.append(s_e)\n            dst_nodes.append(d_e)\n\n        if len(src_nodes) > 0:\n            all_src = cp.concatenate(src_nodes)\n            all_dst = cp.concatenate(dst_nodes)\n            data = cp.ones(len(all_src), dtype=cp.float32)\n            adj = cpx.coo_matrix((data, (all_src, all_dst)), shape=(self.N, self.N))\n            n_comps, labels = cpx_graph.connected_components(adj, directed=False)\n        else:\n            n_comps = self.N\n            labels = cp.arange(self.N, dtype=cp.int32)\n\n        if verbose:\n            comp_sizes = cp.bincount(labels)\n            sorted_sizes = cp.sort(comp_sizes)[::-1]\n            top20 = sorted_sizes[:20].get() if hasattr(sorted_sizes, \'get\') else sorted_sizes[:20]\n            print(f"Phase 1: {n_comps} clusters. Top 20 sizes: {top20}")\n\n        self._run_dynamics(labels, n_comps, omega)\n        \n        # --- PHASE 2: UNSAT Percolation + ErdÅ‘s-RÃ©nyi Super-Clustering ---\n        \n        c_spins = self.sigma[self.lits_idx]\n        lit_is_sat = (c_spins == self.lits_sign)\n        is_unsat = (cp.sum(lit_is_sat, axis=1) == 0)\n        \n        if cp.any(is_unsat):\n            omega_2 = 8.0 * omega\n            P_2 = 1.0 - cp.exp(-omega_2)\n            idx_U = cp.where(is_unsat)[0]\n            n_unsat = len(idx_U)\n            r_vals_U = cp.random.random(n_unsat, dtype=cp.float32)\n            src_2, dst_2 = [], []\n            P_7 = P_2 / 7.0\n            \n            mask_full = (r_vals_U >= 6.0 * P_7) & (r_vals_U < P_2)\n            if cp.any(mask_full):\n                l = self.lits_idx[idx_U[mask_full]]\n                src_2.append(l[:,0]); dst_2.append(l[:,1])\n                src_2.append(l[:,1]); dst_2.append(l[:,2])\n            \n            mask_e0 = (r_vals_U < 2.0 * P_7)\n            if cp.any(mask_e0):\n                l = self.lits_idx[idx_U[mask_e0]]\n                src_2.append(l[:,0]); dst_2.append(l[:,1])\n                \n            mask_e1 = (r_vals_U >= 2.0 * P_7) & (r_vals_U < 4.0 * P_7)\n            if cp.any(mask_e1):\n                l = self.lits_idx[idx_U[mask_e1]]\n                src_2.append(l[:,1]); dst_2.append(l[:,2])\n                \n            mask_e2 = (r_vals_U >= 4.0 * P_7) & (r_vals_U < 6.0 * P_7)\n            if cp.any(mask_e2):\n                l = self.lits_idx[idx_U[mask_e2]]\n                src_2.append(l[:,2]); dst_2.append(l[:,0])\n                \n            # Build Graph Phase 2\n            if len(src_2) > 0:\n                all_src_2 = cp.concatenate(src_2)\n                all_dst_2 = cp.concatenate(dst_2)\n                data_2 = cp.ones(len(all_src_2), dtype=cp.float32)\n                adj_2 = cpx.coo_matrix((data_2, (all_src_2, all_dst_2)), shape=(self.N, self.N))\n                n_comps_2, labels_2 = cpx_graph.connected_components(adj_2, directed=False)\n            else:\n                n_comps_2 = self.N\n                labels_2 = cp.arange(self.N, dtype=cp.int32)\n                \n            # --- ERDOS-RENYI SUPER-CLUSTERING (STRICT FILTER) ---\n            # 1. Identify active variables (those in UNSAT clauses)\n            # Actually, active variables are those involved in the percolation edges?\n            # Or just those in the UNSAT clauses?\n            # User requirement: "variables appearing in at least one UNSAT clause"\n            \n            # Get all vars in UNSAT clauses\n            unsat_vars = self.lits_idx[idx_U].flatten()\n            \n            # Get their clusters\n            active_clusters = labels_2[unsat_vars]\n            unique_active = cp.unique(active_clusters)\n            \n            m = len(unique_active)\n            final_labels = labels_2 # Default\n            final_n_comps = n_comps_2 # Default\n            \n            if m > 1 and self.a > 0:\n                # Map active clusters to 0..m-1\n                # We need a remapping array.\n                # Assuming labels are 0..N-1, we can use a lookup array if N is not too huge.\n                # Or use searchsorted if sparse. N=10000 is small.\n                \n                # Create a map: old_label -> new_label\n                # Initialize with -1\n                cluster_map = cp.full(n_comps_2, -1, dtype=cp.int32)\n                cluster_map[unique_active] = cp.arange(m, dtype=cp.int32)\n                \n                num_edges = int(self.a * (m - 1) / 2)\n                if num_edges > 0:\n                    s_er = cp.random.randint(0, m, size=num_edges, dtype=cp.int32)\n                    d_er = cp.random.randint(0, m, size=num_edges, dtype=cp.int32)\n                    data_er = cp.ones(num_edges, dtype=cp.float32)\n                    adj_er = cpx.coo_matrix((data_er, (s_er, d_er)), shape=(m, m))\n                    \n                    n_super, super_labels = cpx_graph.connected_components(adj_er, directed=False)\n                    \n                    # super_labels is size m (for each active cluster)\n                    # We need to map back to variables.\n                    \n                    # Variables in active clusters get super_label\n                    # Variables in inactive clusters keep their old label (but we need to shift/offset them so they don\'t collide?)\n                    # Or simpler: The kernel only runs on valid_clusters.\n                    # If we merge active clusters into super clusters, we can just update the labels for active variables.\n                    # Be careful about label collision.\n                    # Best way:\n                    # New labels = super_labels[cluster_map[old_label]] IF old_label in active\n                    # ELSE = old_label + offset (to keep distinct)\n                    \n                    # To simplify, we can just say:\n                    # The dynamics only run on the Super Clusters?\n                    # "De mÃªme pour les superclusters." -> implying we only care about these.\n                    \n                    # So, we only care about \'final_labels\' for the active vars.\n                    # But the kernel needs a full labels array for all N vars.\n                    # Inactive vars shouldn\'t flip?\n                    # If we leave them as singletons, they might flip if selected.\n                    # But if they are not in UNSAT clauses, they are "satisfied" locally? No, not necessarily.\n                    # But the requirement is "Second phase percolation ONLY on vars in UNSAT clauses".\n                    \n                    # So, we construct \'final_labels\'.\n                    # For active vars: mapped to super_cluster ID.\n                    # For inactive vars: mapped to a unique ID that is NOT in the active set.\n                    # E.g. make them their own singletons offset by n_super.\n                    \n                    # Efficient mapping:\n                    # 1. Get labels_2 for all vars.\n                    # 2. Check if label is in active set.\n                    #    - We have cluster_map: cluster_map[label] != -1 means active.\n                    mapped_ids = cluster_map[labels_2] # Size N. -1 if inactive.\n                    \n                    # Active ones get super_labels[mapped_ids]\n                    # Inactive ones get n_super + index? Or keep old label + offset?\n                    # Let\'s keep them as distinct.\n                    \n                    is_active = (mapped_ids != -1)\n                    \n                    new_labels = cp.zeros(self.N, dtype=cp.int32)\n                    \n                    # Active vars\n                    new_labels[is_active] = super_labels[mapped_ids[is_active]]\n                    \n                    # Inactive vars: We must ensure they don\'t merge with 0..n_super-1\n                    # We can assign them unique negative IDs or offset IDs.\n                    # Let\'s verify what the kernel does. It iterates valid_clusters.\n                    # If labels[v] is not in valid_clusters, v is never flipped?\n                    # The kernel picks a cluster \'c_id\' from valid_clusters.\n                    # Then loops over clauses. Check: "int cl0 = lit_clusters[idx0];"\n                    # "p_sig0 = (cl0 == c_id) ? -sig0 : sig0;"\n                    # So if cl0 is never c_id, it never flips.\n                    \n                    # So strategy:\n                    # 1. Active vars get labels 0..n_super-1.\n                    # 2. Inactive vars get label -1 (or something distinct).\n                    # 3. Valid clusters = 0..n_super-1.\n                    \n                    new_labels[:] = -1\n                    new_labels[is_active] = super_labels[mapped_ids[is_active]]\n                    \n                    final_labels = new_labels\n                    final_n_comps = n_super # Only count the active super clusters\n            \n            if verbose:\n                # Stats only on active super clusters\n                active_mask = (final_labels != -1)\n                if cp.any(active_mask):\n                    comp_sizes_2 = cp.bincount(final_labels[active_mask])\n                    sorted_sizes_2 = cp.sort(comp_sizes_2)[::-1]\n                    top20_2 = sorted_sizes_2[:20].get() if hasattr(sorted_sizes_2, \'get\') else sorted_sizes_2[:20]\n                    print(f"Phase 2: {n_comps_2} clusters -> {final_n_comps} super-clusters (Active). Top 20 sizes: {top20_2}")\n                else:\n                    print("Phase 2: No active clusters.")\n\n            self._run_dynamics(final_labels, final_n_comps, omega_2)\n\n        e = self.energy_check()\n        if e < self.min_energy:\n            self.min_energy = e\n            self.best_sigma = self.sigma.copy()\n            if e == 0.0:\n                print(f"ðŸŽ‰ SOLUTION FOUND ! (Energy = 0.0) ðŸŽ‰")\n        \n        return e, 0.0, 0.0'
add_code(code_source_3, execution_count=None, outputs=[])

# Cell 4 (Code)
code_source_4 = '# @title 3b. The New Solver: `SwendsenWangGlauberGPU` (Optimized Kernel)\n\n# CUDA Kernel for Glauber Dynamics\nmetropolis_kernel_code = r\'\'\'\n#include <curand_kernel.h>\n\nextern "C" __global__ void run_metropolis_dynamics(\n    signed char* sigma,           // N+1\n    const int* c2c_indptr,        // n_comps + 1\n    const int* c2c_indices,       // n_clauses_refs\n    const int* c2v_indptr,        // n_comps + 1\n    const int* c2v_indices,       // n_vars_refs\n    const int* lits_idx,          // M * 3\n    const signed char* lits_sign, // M * 3\n    const int* lit_clusters,      // M * 3\n    const int* valid_clusters,    // num_valid\n    int num_valid,\n    int steps,\n    float omega,\n    float beta_scale,\n    unsigned long long seed\n) {\n    // Shared memory for reduction and communication\n    __shared__ int delta_E_shared;\n    __shared__ int decision_shared; // 0=reject, 1=accept\n    __shared__ int target_cluster_shared;\n\n    // Initialize RNG\n    curandState state;\n    if (threadIdx.x == 0) {\n        curand_init(seed, 0, 0, &state);\n    }\n\n    for (int step = 0; step < steps; step++) {\n        __syncthreads();\n\n        // --- 1. Pick Target Cluster ---\n        if (threadIdx.x == 0) {\n            delta_E_shared = 0;\n            decision_shared = 0;\n            unsigned int r = curand(&state);\n            int r_idx = r % num_valid;\n            target_cluster_shared = valid_clusters[r_idx];\n        }\n        __syncthreads();\n\n        int c_id = target_cluster_shared;\n        int start_c = c2c_indptr[c_id];\n        int end_c = c2c_indptr[c_id+1];\n\n        // --- 2. Compute Delta E (Parallel over clauses) ---\n        if (start_c < end_c) {\n            for (int i = start_c + threadIdx.x; i < end_c; i += blockDim.x) {\n                int clause_idx = c2c_indices[i];\n\n                int idx0 = clause_idx * 3 + 0;\n                int idx1 = clause_idx * 3 + 1;\n                int idx2 = clause_idx * 3 + 2;\n\n                int l0 = lits_idx[idx0];\n                int l1 = lits_idx[idx1];\n                int l2 = lits_idx[idx2];\n\n                signed char s0 = lits_sign[idx0];\n                signed char s1 = lits_sign[idx1];\n                signed char s2 = lits_sign[idx2];\n\n                signed char sig0 = sigma[l0];\n                signed char sig1 = sigma[l1];\n                signed char sig2 = sigma[l2];\n\n                int cl0 = lit_clusters[idx0];\n                int cl1 = lit_clusters[idx1];\n                int cl2 = lit_clusters[idx2];\n\n                bool sat_curr = (sig0 == s0) || (sig1 == s1) || (sig2 == s2);\n\n                signed char p_sig0 = (cl0 == c_id) ? -sig0 : sig0;\n                signed char p_sig1 = (cl1 == c_id) ? -sig1 : sig1;\n                signed char p_sig2 = (cl2 == c_id) ? -sig2 : sig2;\n\n                bool sat_new = (p_sig0 == s0) || (p_sig1 == s1) || (p_sig2 == s2);\n\n                if (sat_curr != sat_new) {\n                    int local_delta = (int)sat_curr - (int)sat_new;\n                    atomicAdd(&delta_E_shared, local_delta);\n                }\n            }\n        }\n        __syncthreads();\n\n        // --- 3. Decision ---\n        if (threadIdx.x == 0) {\n            int dE = delta_E_shared;\n            if (dE <= 0) {\n                decision_shared = 1;\n            } else {\n                float p = expf(-(float)dE * omega * beta_scale);\n                float r = curand_uniform(&state);\n                if (r < p) {\n                    decision_shared = 1;\n                }\n            }\n        }\n        __syncthreads();\n\n        // --- 4. Update Sigma ---\n        if (decision_shared) {\n            int start_v = c2v_indptr[c_id];\n            int end_v = c2v_indptr[c_id+1];\n            for (int i = start_v + threadIdx.x; i < end_v; i += blockDim.x) {\n                int var_idx = c2v_indices[i];\n                sigma[var_idx] *= -1;\n            }\n        }\n    }\n}\n\'\'\'\n\nclass SwendsenWangGlauberGPU:\n    def __init__(self, clauses_np, N, beta_scale=15.0, steps_flips=None, dynamics="Metropolis-Hastings"):\n        self.N = N\n        self.M = len(clauses_np)\n        self.clauses = cp.array(clauses_np)\n        self.GHOST = 0\n        self.beta_scale = beta_scale\n        if steps_flips is None:\n            self.steps_flips = 2 * N\n        else:\n            self.steps_flips = steps_flips\n        self.dynamics = dynamics\n\n        self.lits_idx = cp.ascontiguousarray(cp.abs(self.clauses).astype(cp.int32))\n        self.lits_sign = cp.ascontiguousarray(cp.sign(self.clauses).astype(cp.int8))\n\n        s = self.lits_sign\n        j01 = cp.where(s[:, 0] == s[:, 1], -1, 1)\n        j12 = cp.where(s[:, 1] == s[:, 2], -1, 1)\n        j20 = cp.where(s[:, 2] == s[:, 0], -1, 1)\n        self.J_tri = cp.stack([j01, j12, j20], axis=1).astype(cp.int8)\n\n        self.sigma = cp.random.choice(cp.array([-1, 1], dtype=cp.int8), size=N+1)\n        self.sigma[0] = 1\n        \n        # Best So Far\n        self.best_sigma = self.sigma.copy()\n        self.min_energy = 1.0\n\n        self.kernel = cp.RawKernel(metropolis_kernel_code, \'run_metropolis_dynamics\', options=(\'-std=c++17\',))\n\n    def energy_check(self):\n        spins = self.sigma[self.lits_idx]\n        is_lit_sat = (spins == self.lits_sign)\n        is_clause_sat = cp.any(is_lit_sat, axis=1)\n        return 1.0 - cp.mean(is_clause_sat)\n\n    def step(self, omega, verbose=False):\n        # --- 1. CLUSTERING ---\n        c_spins = self.sigma[self.lits_idx]\n        lit_is_sat = (c_spins == self.lits_sign)\n        num_lit_sat = cp.sum(lit_is_sat, axis=1)\n        is_fully_sat = (num_lit_sat == 3)\n\n        s0, s1, s2 = c_spins[:, 0], c_spins[:, 1], c_spins[:, 2]\n        sat0 = (s0 * s1 * self.J_tri[:, 0] == 1)\n        sat1 = (s1 * s2 * self.J_tri[:, 1] == 1)\n        sat2 = (s2 * s0 * self.J_tri[:, 2] == 1)\n        sat_mask = cp.stack([sat0, sat1, sat2], axis=1)\n        num_sat_tri = cp.sum(sat_mask, axis=1)\n        is_low_energy = (num_sat_tri == 2)\n\n        P = 1.0 - cp.exp(-omega)\n        rand_vals = cp.random.random(self.M, dtype=cp.float32)\n\n        src_nodes = []\n        dst_nodes = []\n\n        # --- B1. Ghost Connections (Fully SAT Clauses) ---\n        # If freeze: connect ALL THREE literals to Ghost (0)\n        mask_G = is_fully_sat & (rand_vals < P)\n        if cp.any(mask_G):\n            idx_G = cp.where(mask_G)[0]\n\n            # We want to connect lits 0, 1, and 2 of these clauses to Ghost.\n            # Get all literals for these clauses: shape (K, 3) -> flatten to (3K,)\n            targets = self.lits_idx[idx_G].flatten()\n\n            src_nodes.append(cp.zeros_like(targets)) # Connect to Ghost (0)\n            dst_nodes.append(targets)\n\n        # B2. Internal\n        mask_T = is_low_energy & (rand_vals < P)\n        if cp.any(mask_T):\n            idx_T = cp.where(mask_T)[0]\n            r_vals_T = rand_vals[idx_T]\n            sub_sat = sat_mask[idx_T]\n            idx_1st = cp.argmax(sub_sat, axis=1)\n            idx_sum = cp.sum(sub_sat * cp.array([0, 1, 2], dtype=cp.int8), axis=1)\n            idx_2nd = idx_sum - idx_1st\n            P_2 = P / 2.0\n            pick_first = (r_vals_T < P_2)\n            chosen_edge_idx = cp.where(pick_first, idx_1st, idx_2nd)\n            lits = self.lits_idx[idx_T]\n            l0, l1, l2 = lits[:,0], lits[:,1], lits[:,2]\n            s_e = cp.where(chosen_edge_idx==0, l0, cp.where(chosen_edge_idx==1, l1, l2))\n            d_e = cp.where(chosen_edge_idx==0, l1, cp.where(chosen_edge_idx==1, l2, l0))\n            src_nodes.append(s_e)\n            dst_nodes.append(d_e)\n\n        # Connected Components\n        if len(src_nodes) > 0:\n            all_src = cp.concatenate(src_nodes)\n            all_dst = cp.concatenate(dst_nodes)\n            data = cp.ones(len(all_src), dtype=cp.float32)\n            adj = cpx.coo_matrix((data, (all_src, all_dst)), shape=(self.N+1, self.N+1), dtype=cp.float32)\n            n_comps, labels = cpx_graph.connected_components(adj, directed=False)\n        else:\n            n_comps = self.N + 1\n            labels = cp.arange(self.N + 1, dtype=cp.int32)\n\n        # Stats\n        comp_sizes = cp.bincount(labels)\n        sorted_sizes = cp.sort(comp_sizes)[::-1]\n        c1_frac = sorted_sizes[0] / (self.N + 1)\n        c2_frac = sorted_sizes[1] / (self.N + 1) if n_comps > 1 else 0.0\n        if verbose:\n            print(f"Phase 1 Top 7 Clusters: {sorted_sizes[:7]}")\n\n        # --- 2. DYNAMICS (KERNEL) ---\n        lit_clusters = labels[self.lits_idx] # (M, 3)\n\n        # CSR: Cluster -> Vars\n        data_v = cp.ones(self.N + 1, dtype=cp.bool_)\n        cluster_to_vars = cpx.coo_matrix(\n            (data_v, (labels, cp.arange(self.N + 1))),\n            shape=(n_comps, self.N + 1)\n        ).tocsr()\n\n        # CSR: Cluster -> Clauses\n        flat_clusters = lit_clusters.flatten()\n        flat_clauses = cp.repeat(cp.arange(self.M), 3)\n        combined_keys = flat_clusters.astype(cp.int64) * self.M + flat_clauses.astype(cp.int64)\n        unique_keys = cp.unique(combined_keys)\n        u_clusters = (unique_keys // self.M).astype(cp.int32)\n        u_clauses = (unique_keys % self.M).astype(cp.int32)\n        data_c = cp.ones(len(u_clusters), dtype=cp.bool_)\n\n        cluster_to_clauses = cpx.coo_matrix(\n            (data_c, (u_clusters, u_clauses)),\n            shape=(n_comps, self.M)\n        ).tocsr()\n\n        # Valid Clusters\n        ghost_label = labels[0]\n        unique_labels = cp.unique(labels)\n        valid_clusters = unique_labels[unique_labels != ghost_label].astype(cp.int32)\n        num_valid = len(valid_clusters)\n\n        if num_valid > 0:\n            c2c_indptr = cluster_to_clauses.indptr.astype(cp.int32)\n            c2c_indices = cluster_to_clauses.indices.astype(cp.int32)\n            c2v_indptr = cluster_to_vars.indptr.astype(cp.int32)\n            c2v_indices = cluster_to_vars.indices.astype(cp.int32)\n\n            lit_clusters_ptr = cp.ascontiguousarray(lit_clusters.astype(cp.int32))\n\n            seed = int(time.time() * 1000) % 1000000007\n\n            self.kernel(\n                (1,), (256,),\n                (\n                    self.sigma,\n                    c2c_indptr, c2c_indices,\n                    c2v_indptr, c2v_indices,\n                    self.lits_idx, self.lits_sign,\n                    lit_clusters_ptr,\n                    valid_clusters,\n                    cp.int32(num_valid),\n                    cp.int32(self.steps_flips),\n                    cp.float32(omega),\n                    cp.float32(self.beta_scale),\n                    cp.uint64(seed)\n                )\n            )\n\n        # --- PHASE 2: UNSAT DYNAMICS ---\n        # 1. Re-evaluate Status after Phase 1\n        c_spins = self.sigma[self.lits_idx]\n        lit_is_sat = (c_spins == self.lits_sign)\n        num_lit_sat = cp.sum(lit_is_sat, axis=1)\n        is_unsat = (num_lit_sat == 0)\n\n        P = 1.0 - cp.exp(-omega)\n        # 2. Percolation on UNSAT Clauses ONLY\n        src_nodes_2 = []\n        dst_nodes_2 = []\n\n        if cp.any(is_unsat):\n            idx_U = cp.where(is_unsat)[0]\n            n_unsat = len(idx_U)\n            r_vals_U = cp.random.random(n_unsat, dtype=cp.float32)\n            \n            P_7 = P / 7.0\n            \n            # --- FULL FREEZE (Two edges => Triangle) ---\n            # Range [6P/7, P)\n            mask_full = (r_vals_U >= 6.0 * P_7) & (r_vals_U < P)\n            if cp.any(mask_full):\n                sub_idx = idx_U[mask_full]\n                lits = self.lits_idx[sub_idx]\n                # Edge 0-1\n                src_nodes_2.append(lits[:, 0])\n                dst_nodes_2.append(lits[:, 1])\n                # Edge 1-2\n                src_nodes_2.append(lits[:, 1])\n                dst_nodes_2.append(lits[:, 2])\n            \n            # --- SINGLE EDGE FREEZE ---\n            # Edge 0 (0-1) : [0, 2P/7)\n            mask_e0 = (r_vals_U < 2.0 * P_7)\n            if cp.any(mask_e0):\n                sub_idx = idx_U[mask_e0]\n                lits = self.lits_idx[sub_idx]\n                src_nodes_2.append(lits[:, 0])\n                dst_nodes_2.append(lits[:, 1])\n            # Edge 1 (1-2) : [2P/7, 4P/7)\n            mask_e1 = (r_vals_U >= 2.0 * P_7) & (r_vals_U < 4.0 * P_7)\n            if cp.any(mask_e1):\n                sub_idx = idx_U[mask_e1]\n                lits = self.lits_idx[sub_idx]\n                src_nodes_2.append(lits[:, 1])\n                dst_nodes_2.append(lits[:, 2])\n            # Edge 2 (2-0) : [4P/7, 6P/7)\n            mask_e2 = (r_vals_U >= 4.0 * P_7) & (r_vals_U < 6.0 * P_7)\n            if cp.any(mask_e2):\n                sub_idx = idx_U[mask_e2]\n                lits = self.lits_idx[sub_idx]\n                src_nodes_2.append(lits[:, 2])\n                dst_nodes_2.append(lits[:, 0])\n\n        # 3. Clustering Phase 2\n        if len(src_nodes_2) > 0:\n            all_src_2 = cp.concatenate(src_nodes_2)\n            all_dst_2 = cp.concatenate(dst_nodes_2)\n            data_2 = cp.ones(len(all_src_2), dtype=cp.float32)\n            adj_2 = cpx.coo_matrix((data_2, (all_src_2, all_dst_2)), shape=(self.N+1, self.N+1), dtype=cp.float32)\n            n_comps_2, labels_2 = cpx_graph.connected_components(adj_2, directed=False)\n        else:\n            n_comps_2 = self.N + 1\n            labels_2 = cp.arange(self.N + 1, dtype=cp.int32)\n\n        comp_sizes_2 = cp.bincount(labels_2)\n        if verbose:\n            sorted_sizes_2 = cp.sort(comp_sizes_2)[::-1]\n            print(f"Phase 2 Top 7 Clusters: {sorted_sizes_2[:7]}")\n\n        # 4. Prepare Kernel Phase 2\n        lit_clusters_2 = labels_2[self.lits_idx]\n        data_v_2 = cp.ones(self.N + 1, dtype=cp.bool_)\n        cluster_to_vars_2 = cpx.coo_matrix((data_v_2, (labels_2, cp.arange(self.N + 1))), shape=(n_comps_2, self.N + 1)).tocsr()\n        flat_clusters_2 = lit_clusters_2.flatten()\n        flat_clauses_2 = cp.repeat(cp.arange(self.M), 3)\n        combined_keys_2 = flat_clusters_2.astype(cp.int64) * self.M + flat_clauses_2.astype(cp.int64)\n        unique_keys_2 = cp.unique(combined_keys_2)\n        u_clusters_2 = (unique_keys_2 // self.M).astype(cp.int32)\n        u_clauses_2 = (unique_keys_2 % self.M).astype(cp.int32)\n        data_c_2 = cp.ones(len(u_clusters_2), dtype=cp.bool_)\n        cluster_to_clauses_2 = cpx.coo_matrix((data_c_2, (u_clusters_2, u_clauses_2)), shape=(n_comps_2, self.M)).tocsr()\n\n        # Valid Clusters 2: Size > 1 AND not Ghost\n        ghost_label_2 = labels_2[0]\n        unique_labels_2 = cp.unique(labels_2)\n        mask_valid = (comp_sizes_2[unique_labels_2] > 1) & (unique_labels_2 != ghost_label_2)\n        valid_clusters_2 = unique_labels_2[mask_valid].astype(cp.int32)\n        num_valid_2 = len(valid_clusters_2)\n\n        # 5. Execute Kernel Phase 2\n        if num_valid_2 > 0:\n            c2c_indptr_2 = cluster_to_clauses_2.indptr.astype(cp.int32)\n            c2c_indices_2 = cluster_to_clauses_2.indices.astype(cp.int32)\n            c2v_indptr_2 = cluster_to_vars_2.indptr.astype(cp.int32)\n            c2v_indices_2 = cluster_to_vars_2.indices.astype(cp.int32)\n            lit_clusters_ptr_2 = cp.ascontiguousarray(lit_clusters_2.astype(cp.int32))\n            self.kernel((1,), (256,), (self.sigma, c2c_indptr_2, c2c_indices_2, c2v_indptr_2, c2v_indices_2, self.lits_idx, self.lits_sign, lit_clusters_ptr_2, valid_clusters_2, cp.int32(num_valid_2), cp.int32(self.steps_flips), cp.float32(omega), cp.float32(self.beta_scale), cp.uint64(seed + 1)))\n\n        current_energy = self.energy_check()\n        \n        # Update Best So Far\n        if current_energy < self.min_energy:\n            self.min_energy = current_energy\n            self.best_sigma = self.sigma.copy()\n            if self.min_energy == 0.0:\n                print(f"ðŸŽ‰ SOLUTION FOUND ! (Energy = 0.0) ðŸŽ‰")\n        \n        return current_energy, c1_frac, c2_frac'
add_code(code_source_4, execution_count=None, outputs=[])

# Cell 5 (Code)
code_source_5 = '# @title 3c. The Solver: `CompleteSwendsenWangGPU`\n\nclass CompleteSwendsenWangGPU:\n    def __init__(self, clauses_np, N, beta_scale=15.0, steps_flips=None, dynamics="Metropolis-Hastings"):\n        self.N = N\n        self.M = len(clauses_np)\n        self.clauses = cp.array(clauses_np)\n        self.beta_scale = beta_scale\n        if steps_flips is None:\n            self.steps_flips = 2 * N\n        else:\n            self.steps_flips = steps_flips\n        self.dynamics = dynamics\n\n        self.lits_idx = cp.ascontiguousarray(cp.abs(self.clauses).astype(cp.int32))\n        self.lits_sign = cp.ascontiguousarray(cp.sign(self.clauses).astype(cp.int8))\n\n        self.sigma = cp.random.choice(cp.array([-1, 1], dtype=cp.int8), size=N+1)\n        self.sigma[0] = 1 # Dummy index 0\n        \n        self.best_sigma = self.sigma.copy()\n        self.min_energy = 1.0\n\n        # Reusing the Glauber kernel\n        self.kernel = cp.RawKernel(metropolis_kernel_code, \'run_metropolis_dynamics\', options=(\'-std=c++17\',))\n\n    def energy_check(self):\n        spins = self.sigma[self.lits_idx]\n        is_lit_sat = (spins == self.lits_sign)\n        is_clause_sat = cp.any(is_lit_sat, axis=1)\n        return 1.0 - cp.mean(is_clause_sat)\n\n    def step(self, omega, verbose=False):\n        P = 1.0 - cp.exp(-omega)\n        rand_vals = cp.random.random(self.M, dtype=cp.float32)\n        \n        src_nodes = []\n        dst_nodes = []\n        \n        P_7 = P / 7.0\n        \n        # Range [6P/7, P) -> Full Freeze\n        mask_full = (rand_vals >= 6.0 * P_7) & (rand_vals < P)\n        if cp.any(mask_full):\n            sub_idx = cp.where(mask_full)[0]\n            lits = self.lits_idx[sub_idx]\n            # Edge 0-1\n            src_nodes.append(lits[:, 0])\n            dst_nodes.append(lits[:, 1])\n            # Edge 1-2\n            src_nodes.append(lits[:, 1])\n            dst_nodes.append(lits[:, 2])\n            \n        # Edge 0 (0-1)\n        mask_e0 = (rand_vals < 2.0 * P_7)\n        if cp.any(mask_e0):\n            sub_idx = cp.where(mask_e0)[0]\n            lits = self.lits_idx[sub_idx]\n            src_nodes.append(lits[:, 0])\n            dst_nodes.append(lits[:, 1])\n            \n        # Edge 1 (1-2)\n        mask_e1 = (rand_vals >= 2.0 * P_7) & (rand_vals < 4.0 * P_7)\n        if cp.any(mask_e1):\n            sub_idx = cp.where(mask_e1)[0]\n            lits = self.lits_idx[sub_idx]\n            src_nodes.append(lits[:, 1])\n            dst_nodes.append(lits[:, 2])\n            \n        # Edge 2 (2-0)\n        mask_e2 = (rand_vals >= 4.0 * P_7) & (rand_vals < 6.0 * P_7)\n        if cp.any(mask_e2):\n            sub_idx = cp.where(mask_e2)[0]\n            lits = self.lits_idx[sub_idx]\n            src_nodes.append(lits[:, 2])\n            dst_nodes.append(lits[:, 0])\n\n        if len(src_nodes) > 0:\n            all_src = cp.concatenate(src_nodes)\n            all_dst = cp.concatenate(dst_nodes)\n            data = cp.ones(len(all_src), dtype=cp.float32)\n            adj = cpx.coo_matrix((data, (all_src, all_dst)), shape=(self.N+1, self.N+1), dtype=cp.float32)\n            n_comps, labels = cpx_graph.connected_components(adj, directed=False)\n        else:\n            n_comps = self.N + 1\n            labels = cp.arange(self.N + 1, dtype=cp.int32)\n\n        if verbose:\n            comp_sizes = cp.bincount(labels)\n            sorted_sizes = cp.sort(comp_sizes)[::-1]\n            print(f"Complete SW Top 7 Clusters: {sorted_sizes[:7]}")\n\n        lit_clusters = labels[self.lits_idx]\n        \n        data_v = cp.ones(self.N + 1, dtype=cp.bool_)\n        cluster_to_vars = cpx.coo_matrix((data_v, (labels, cp.arange(self.N + 1))), shape=(n_comps, self.N + 1)).tocsr()\n        \n        flat_clusters = lit_clusters.flatten()\n        flat_clauses = cp.repeat(cp.arange(self.M), 3)\n        combined_keys = flat_clusters.astype(cp.int64) * self.M + flat_clauses.astype(cp.int64)\n        unique_keys = cp.unique(combined_keys)\n        u_clusters = (unique_keys // self.M).astype(cp.int32)\n        u_clauses = (unique_keys % self.M).astype(cp.int32)\n        data_c = cp.ones(len(u_clusters), dtype=cp.bool_)\n        cluster_to_clauses = cpx.coo_matrix((data_c, (u_clusters, u_clauses)), shape=(n_comps, self.M)).tocsr()\n        \n        # Exclude dummy 0\n        ghost_label = labels[0]\n        unique_labels = cp.unique(labels)\n        valid_clusters = unique_labels[unique_labels != ghost_label].astype(cp.int32)\n        num_valid = len(valid_clusters)\n        \n        if num_valid > 0:\n            c2c_indptr = cluster_to_clauses.indptr.astype(cp.int32)\n            c2c_indices = cluster_to_clauses.indices.astype(cp.int32)\n            c2v_indptr = cluster_to_vars.indptr.astype(cp.int32)\n            c2v_indices = cluster_to_vars.indices.astype(cp.int32)\n            lit_clusters_ptr = cp.ascontiguousarray(lit_clusters.astype(cp.int32))\n            \n            seed = int(time.time() * 1000) % 1000000007\n            self.kernel((1,), (256,), (self.sigma, c2c_indptr, c2c_indices, c2v_indptr, c2v_indices, self.lits_idx, self.lits_sign, lit_clusters_ptr, valid_clusters, cp.int32(num_valid), cp.int32(self.steps_flips), cp.float32(omega), cp.float32(self.beta_scale), cp.uint64(seed)))\n            \n        # --- PHASE 2: UNSAT DYNAMICS (8x Boost) ---\n        c_spins = self.sigma[self.lits_idx]\n        lit_is_sat = (c_spins == self.lits_sign)\n        num_lit_sat = cp.sum(lit_is_sat, axis=1)\n        is_unsat = (num_lit_sat == 0)\n\n        if cp.any(is_unsat):\n            omega_2 = 8.0 * omega\n            P_2 = 1.0 - cp.exp(-omega_2)\n            idx_U = cp.where(is_unsat)[0]\n            n_unsat = len(idx_U)\n            r_vals_U = cp.random.random(n_unsat, dtype=cp.float32)\n            \n            src_nodes_2 = []\n            dst_nodes_2 = []\n            \n            P_7 = P_2 / 7.0\n            \n            # [6P/7, P) -> Full Freeze\n            mask_full = (r_vals_U >= 6.0 * P_7) & (r_vals_U < P_2)\n            if cp.any(mask_full):\n                sub_idx = idx_U[mask_full]\n                lits = self.lits_idx[sub_idx]\n                src_nodes_2.append(lits[:, 0])\n                dst_nodes_2.append(lits[:, 1])\n                src_nodes_2.append(lits[:, 1])\n                dst_nodes_2.append(lits[:, 2])\n\n            # [0, 2P/7) -> Edge 0\n            mask_e0 = (r_vals_U < 2.0 * P_7)\n            if cp.any(mask_e0):\n                sub_idx = idx_U[mask_e0]\n                lits = self.lits_idx[sub_idx]\n                src_nodes_2.append(lits[:, 0])\n                dst_nodes_2.append(lits[:, 1])\n\n            # [2P/7, 4P/7) -> Edge 1\n            mask_e1 = (r_vals_U >= 2.0 * P_7) & (r_vals_U < 4.0 * P_7)\n            if cp.any(mask_e1):\n                sub_idx = idx_U[mask_e1]\n                lits = self.lits_idx[sub_idx]\n                src_nodes_2.append(lits[:, 1])\n                dst_nodes_2.append(lits[:, 2])\n\n            # [4P/7, 6P/7) -> Edge 2\n            mask_e2 = (r_vals_U >= 4.0 * P_7) & (r_vals_U < 6.0 * P_7)\n            if cp.any(mask_e2):\n                sub_idx = idx_U[mask_e2]\n                lits = self.lits_idx[sub_idx]\n                src_nodes_2.append(lits[:, 2])\n                dst_nodes_2.append(lits[:, 0])\n\n            if len(src_nodes_2) > 0:\n                all_src_2 = cp.concatenate(src_nodes_2)\n                all_dst_2 = cp.concatenate(dst_nodes_2)\n                data_2 = cp.ones(len(all_src_2), dtype=cp.float32)\n                adj_2 = cpx.coo_matrix((data_2, (all_src_2, all_dst_2)), shape=(self.N+1, self.N+1), dtype=cp.float32)\n                n_comps_2, labels_2 = cpx_graph.connected_components(adj_2, directed=False)\n            else:\n                n_comps_2 = self.N + 1\n                labels_2 = cp.arange(self.N + 1, dtype=cp.int32)\n\n            if verbose:\n                comp_sizes_2 = cp.bincount(labels_2)\n                sorted_sizes_2 = cp.sort(comp_sizes_2)[::-1]\n                print(f"Complete SW Phase 2 (UNSAT) Top 7 Clusters: {sorted_sizes_2[:7]}")\n\n            # Valid Clusters: Variables in UNSAT clauses (including singletons)\n            unsat_vars = self.lits_idx[idx_U].flatten()\n            relevant_clusters = labels_2[unsat_vars]\n            unique_relevant = cp.unique(relevant_clusters)\n            ghost_label_2 = labels_2[0]\n            valid_clusters_2 = unique_relevant[unique_relevant != ghost_label_2].astype(cp.int32)\n            num_valid_2 = len(valid_clusters_2)\n\n            if num_valid_2 > 0:\n                # Setup Kernel Phase 2\n                lit_clusters_2 = labels_2[self.lits_idx]\n                data_v_2 = cp.ones(self.N + 1, dtype=cp.bool_)\n                cluster_to_vars_2 = cpx.coo_matrix((data_v_2, (labels_2, cp.arange(self.N + 1))), shape=(n_comps_2, self.N + 1)).tocsr()\n                flat_clusters_2 = lit_clusters_2.flatten()\n                flat_clauses_2 = cp.repeat(cp.arange(self.M), 3)\n                combined_keys_2 = flat_clusters_2.astype(cp.int64) * self.M + flat_clauses_2.astype(cp.int64)\n                unique_keys_2 = cp.unique(combined_keys_2)\n                u_clusters_2 = (unique_keys_2 // self.M).astype(cp.int32)\n                u_clauses_2 = (unique_keys_2 % self.M).astype(cp.int32)\n                data_c_2 = cp.ones(len(u_clusters_2), dtype=cp.bool_)\n                cluster_to_clauses_2 = cpx.coo_matrix((data_c_2, (u_clusters_2, u_clauses_2)), shape=(n_comps_2, self.M)).tocsr()\n\n                c2c_indptr_2 = cluster_to_clauses_2.indptr.astype(cp.int32)\n                c2c_indices_2 = cluster_to_clauses_2.indices.astype(cp.int32)\n                c2v_indptr_2 = cluster_to_vars_2.indptr.astype(cp.int32)\n                c2v_indices_2 = cluster_to_vars_2.indices.astype(cp.int32)\n                lit_clusters_ptr_2 = cp.ascontiguousarray(lit_clusters_2.astype(cp.int32))\n                \n                # Reuse seed + offset\n                self.kernel((1,), (256,), (self.sigma, c2c_indptr_2, c2c_indices_2, c2v_indptr_2, c2v_indices_2, self.lits_idx, self.lits_sign, lit_clusters_ptr_2, valid_clusters_2, cp.int32(num_valid_2), cp.int32(self.steps_flips), cp.float32(omega_2), cp.float32(self.beta_scale), cp.uint64(seed + 100)))\n\n        current_energy = self.energy_check()\n        if current_energy < self.min_energy:\n            self.min_energy = current_energy\n            self.best_sigma = self.sigma.copy()\n            if self.min_energy == 0.0:\n                print("ðŸŽ‰ COMPLETE SOLUTION FOUND ! (Energy = 0.0) ðŸŽ‰")\n                \n        return current_energy, 0.0, 0.0'
add_code(code_source_5, execution_count=None, outputs=[])

# Cell 6 (Code)
code_source_6 = '# @title 4. Baseline: `WalkSAT` (CPU Optimized)\nclass WalkSAT:\n    def __init__(self, clauses_np, N):\n        self.N = N\n        self.clauses = clauses_np # NumPy (CPU)\n        self.M = len(clauses_np)\n\n        # Precompute lookups for break-count (simplification: simple evaluation)\n        self.vars_in_clauses = [[] for _ in range(N + 1)]\n        for m, clause in enumerate(self.clauses):\n            for lit in clause:\n                self.vars_in_clauses[abs(lit)].append(m)\n\n        # Random init\n        self.sigma = np.random.choice([-1, 1], size=N+1)\n        self.sigma[0] = 1\n\n    def evaluate(self):\n        # Calculate full status\n        # lit > 0: sat if sigma[lit] == 1\n        # lit < 0: sat if sigma[abs(lit)] == -1\n        # lit * sigma[abs(lit)] > 0\n\n        # Vectorized check\n        lits = self.clauses\n        # Get spins\n        s = self.sigma[np.abs(lits)]\n        # Check signs\n        sat = (lits * s) > 0\n        clause_sat = np.any(sat, axis=1)\n        return np.where(~clause_sat)[0], 1.0 - np.mean(clause_sat)\n\n    def step(self, flips=1):\n        # Perform `flips` number of flips\n        # Standard WalkSAT parameters: p = 0.5 (noise)\n        p = 0.5\n\n        unsat_indices, energy = self.evaluate()\n        if len(unsat_indices) == 0:\n            return 0.0 # Solved\n\n        for _ in range(flips):\n            # Pick random unsat clause\n            if len(unsat_indices) == 0: break\n\n            # Simple random selection\n            clause_idx = np.random.choice(unsat_indices)\n            clause = self.clauses[clause_idx]\n            vars_in_clause = np.abs(clause)\n\n            # Decide: Random or Greedy?\n            if np.random.random() < p:\n                # Random variable in clause\n                target = np.random.choice(vars_in_clause)\n            else:\n                # Greedy: Minimize break-count\n                # "If I flip v, how many currently satisfied clauses become unsatisfied?"\n                best_break = float(\'inf\')\n                target = vars_in_clause[0]\n\n                # To be fast, we only check clauses containing these variables\n                for v in vars_in_clause:\n                    break_count = 0\n                    # Check clauses containing v\n                    # This loop is the bottleneck in Python.\n                    # For N=500, simple check is okay.\n\n                    # Flip v temporarily\n                    self.sigma[v] *= -1\n\n                    # Check clauses that contain v\n                    # Ideally we have a list of clauses for v\n                    affected_clauses = self.vars_in_clauses[v]\n\n                    # For these clauses, are they now UNSAT?\n                    # (We only care if they WAS SAT and NOW UNSAT)\n                    # Re-evaluating them is safest\n                    for c_idx in affected_clauses:\n                        c = self.clauses[c_idx]\n                        if not np.any((c * self.sigma[np.abs(c)]) > 0):\n                            break_count += 1\n\n                    # Restore\n                    self.sigma[v] *= -1\n\n                    if break_count < best_break:\n                        best_break = break_count\n                        target = v\n                    elif break_count == best_break:\n                        # Tie-breaking\n                        if np.random.random() < 0.5:\n                            target = v\n\n            # Flip chosen target\n            self.sigma[target] *= -1\n\n            # Re-eval full unsat list periodically or locally update?\n            # For simplicity in this demo, we re-eval full list every flip is too slow?\n            # No, for comparison curve, we run K flips then measure.\n\n            # We don\'t update unsat_indices inside this tight loop for speed,\n            # we just accept we might pick a now-satisfied clause if we don\'t update?\n            # Standard WalkSAT updates the state.\n            # To emulate speed, we won\'t re-calculate the full UNSAT list every micro-step.\n            # We rely on the fact that we pick from the list we had.\n            # But flipping fixes some and breaks others.\n            # Valid WalkSAT implementation requires updating logic.\n\n            # Let\'s trust the "Batch" approach:\n            # We assume we just do 1 flip properly per call to this function?\n            # No, user wants performance comparison.\n            # Let\'s do a simplified noise step: Just pick random UNSAT and flip random var.\n            # This is "Random Walk" (pure noise), weaker than WalkSAT but faster to code.\n            # Real WalkSAT is greedy.\n\n            pass # (Logic moved to loop below)\n\n        # Re-run proper logic for the batch\n        # We will implement a simplified version: Random Walk on UNSAT variables (GSAT-like)\n        # Or just 1 Greedy flip.\n\n        # Let\'s do 1 Greedy Flip per \'step\' call, but call it N times in the loop?\n        # No, too slow overhead.\n\n        # Proper Python implementation is hard to make fast.\n        # Let\'s return the energy after doing `flips` random valid moves.\n\n        current_unsat, _ = self.evaluate()\n        if len(current_unsat) == 0: return 0.0\n\n        # Fast "ProbSAT" style:\n        # Pick clause -> Pick var based on make/break distribution\n        # Here: Pure Random Walk (Noise=1.0) is a baseline.\n\n        target_clause = np.random.choice(current_unsat)\n        vars_c = np.abs(self.clauses[target_clause])\n        # Heuristic: Pick var that appears in fewest other satisfied clauses?\n        # Let\'s just pick Random variable in clause (Noise=1.0)\n        # This is surprisingly effective for Random 3-SAT.\n        v_flip = np.random.choice(vars_c)\n        self.sigma[v_flip] *= -1\n\n        _, e = self.evaluate()\n        return e'
add_code(code_source_6, execution_count=None, outputs=[])

# Cell 7 (Code)
code_source_7 = '# @title 5. Main Simulation Loop\nN = 10000\nalpha = 4.0 # 4.26 transition\nclauses_np, _ = generate_random_3sat(N, alpha, seed=42)\nprint(f"Instance: N={N}, M={len(clauses_np)}, Alpha={alpha}")\n\n# Solvers\nsolver_er = SwendsenWangErdosRenyiGPU(clauses_np, N, beta_scale=75.0, steps_flips=2*N, a=1.0)\nsolver_gl = SwendsenWangGlauberGPU(clauses_np, N, beta_scale=75.0, steps_flips=2*N)\nsolver_complete = CompleteSwendsenWangGPU(clauses_np, N, beta_scale=75.0, steps_flips=2*N)\nwalksat = WalkSAT(clauses_np, N)\n\nsteps = 10000\nomega_min = 0.1\nomega_max = 0.2\n\nepsilon = 1e-4\nraw_decay = np.geomspace(1, epsilon, steps)\ndecay_01 = (raw_decay - epsilon) / (1.0 - epsilon)\nomega_schedule = omega_max - (omega_max - omega_min) * decay_01\n\n# History\nhistory_er = []\nhistory_er_c1 = []\n\nhistory_gl = [] # Glauber\nhistory_gl_c1 = []\n\nhistory_cp = [] # Complete\nhistory_cp_c1 = []\n\nhistory_ws = []\n\nt0 = time.time()\nprint("Starting Comparison...")\n\nfor i, omega in enumerate(omega_schedule):\n    is_verbose = (i % 50 == 0)\n    \n    # 1. ER-SW (New O(m) Super-Cluster) - Verbose enabled for debug\n    unsat_er, c1_er, _ = solver_er.step(omega, verbose=is_verbose)\n    if hasattr(unsat_er, \'get\'): history_er.append(float(unsat_er.get()))\n    else: history_er.append(float(unsat_er))\n    if hasattr(c1_er, \'get\'): history_er_c1.append(float(c1_er.get()))\n    else: history_er_c1.append(float(c1_er))\n\n    # 2. SW Glauber (Reference)\n    unsat_gl, c1_gl, _ = solver_gl.step(omega, verbose=False)\n    if hasattr(unsat_gl, \'get\'): history_gl.append(float(unsat_gl.get()))\n    else: history_gl.append(float(unsat_gl))\n    if hasattr(c1_gl, \'get\'): history_gl_c1.append(float(c1_gl.get()))\n    else: history_gl_c1.append(float(c1_gl))\n    \n    # 3. Complete SW (Reference)\n    unsat_cp, c1_cp, _ = solver_complete.step(omega, verbose=False)\n    if hasattr(unsat_cp, \'get\'): history_cp.append(float(unsat_cp.get()))\n    else: history_cp.append(float(unsat_cp))\n    if hasattr(c1_cp, \'get\'): history_cp_c1.append(float(c1_cp.get()))\n    else: history_cp_c1.append(float(c1_cp))\n\n    # 4. WalkSAT\n    flips_per_step = N // 10000\n    if flips_per_step < 1: flips_per_step = 1\n    e_ws = 1.0\n    for _ in range(flips_per_step):\n        e_ws = walksat.step(flips=1)\n        if e_ws == 0.0: break\n    history_ws.append(e_ws)\n\n    if is_verbose:\n        print(f"Step {i:4d} | w={omega:.3f} | ER: {unsat_er:.5f} | GL: {unsat_gl:.5f} | CP: {unsat_cp:.5f} | WS: {e_ws:.5f}")\n        if unsat_er == 0.0: print("ER SOLVED!"); break\n        if unsat_gl == 0.0: print("GL SOLVED!"); break\n        if unsat_cp == 0.0: print("CP SOLVED!"); break\n        if e_ws == 0.0: print("WS SOLVED!"); break\n\ndt = time.time() - t0\nprint(f"Done in {dt:.2f}s")\n\n# Plot\nomega_cpu = omega_schedule[:len(history_er)]\ner_cpu = np.array(history_er)\ngl_cpu = np.array(history_gl)\ncp_cpu = np.array(history_cp)\nws_cpu = np.array(history_ws)\n\nplt.figure(figsize=(12, 7))\nax1 = plt.gca()\n\n# Energy Axis\nl1, = ax1.plot(omega_cpu, er_cpu, label=\'Erdos-Renyi SW (Ours)\', color=\'cyan\', linewidth=2)\nl2, = ax1.plot(omega_cpu, gl_cpu, label=\'SW + Glauber\', color=\'lime\', linewidth=1.5, linestyle=\'--\')\nl3, = ax1.plot(omega_cpu, cp_cpu, label=\'Complete SW\', color=\'yellow\', linewidth=1.5, linestyle=\'--\')\nl4, = ax1.plot(omega_cpu, ws_cpu, label=\'WalkSAT\', color=\'red\', alpha=0.5)\n\nax1.set_xlabel(r\'Coupling $\\omega$ (Time)\')\nax1.set_ylabel(\'Fraction Unsatisfied\', color=\'white\')\nax1.tick_params(axis=\'y\', labelcolor=\'white\')\nax1.grid(True, alpha=0.2)\n\n# Cluster Axis\nax2 = ax1.twinx()\n# l5, = ax2.plot(omega_cpu, history_er_c1, label=\'Cluster (ER)\', color=\'magenta\', linestyle=\':\', alpha=0.5)\n\nax2.set_ylabel(\'Cluster Size\', color=\'white\')\nax2.tick_params(axis=\'y\', labelcolor=\'white\')\n\n# Legend\nlines = [l1, l2, l3, l4]\nlabels = [l.get_label() for l in lines]\nax1.legend(lines, labels, loc=\'upper right\')\n\nplt.title(f\'Solver Comparison (N={N}, Alpha={alpha})\')\nplt.show()'
add_code(code_source_7, execution_count=None, outputs=[{"output_type": "stream", "name": "stdout", "text": ["Instance: N=10000, M=40000, Alpha=4\n"]}])

with open("Swendsen-Wang_3SAT_Colab.ipynb", "w", encoding="utf-8") as f:
    json.dump(notebook, f, indent=1)

print("Notebook generated successfully.")
