{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0145748b",
   "metadata": {},
   "source": [
    "# Higher-Order Monte Carlo Cluster Dynamics for 3-SAT (GPU)\n",
    "\n",
    "This notebook implements a high-performance **Swendsen-Wang Cluster Dynamics** solver for 3-SAT problems, adapted from the physics of **spatially embedded graphs** and **frustrated systems** (referencing *SODA 2026* and *Asilomar 2025*).\n",
    "\n",
    "## The Physics\n",
    "Instead of treating SAT clauses as simple constraints, we map them to **Tetrahedrons** (4-body interactions). By distributing energy onto these higher-order structures and utilizing a specific decision tree for bond percolation, we can:\n",
    "1.  Minimize the number of \"frozen\" bonds (reducing frustration).\n",
    "2.  Maintain the correct Gibbs measure.\n",
    "3.  Accelerate sampling via cluster updates.\n",
    "\n",
    "## Algorithm Architecture\n",
    "The implementation follows a strict **Array Programming** paradigm using **CuPy** (CUDA for Python) to ensure massive parallelism.\n",
    "\n",
    "*   **Mapping:** 3-SAT Clauses $\\to$ Tetrahedrons (via Ghost/Slack nodes).\n",
    "*   **Dynamics:** 3-State Bond Sampling ($B=0, 1, 2$) based on satisfaction levels $k$.\n",
    "*   **Witness Selection:** Vectorized \"Random Priority\" mechanism (no loops).\n",
    "*   **Cluster Flipping:** Ghost-Node Graph construction + Connected Components on GPU.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8106a8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title 1. Environment Setup & Imports\n",
    "# We check for GPU availability and ensure the CuPy version matches the Driver.\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import subprocess\n",
    "import time\n",
    "import warnings\n",
    "import requests\n",
    "\n",
    "# Function to force install a compatible CuPy\n",
    "def install_compatible_cupy():\n",
    "    print(\"Installing cupy-cuda11x (broad compatibility)...\")\n",
    "    try:\n",
    "        subprocess.run([sys.executable, '-m', 'pip', 'uninstall', '-y', 'cupy', 'cupy-cuda12x', 'cupy-cuda11x'], check=True)\n",
    "        subprocess.run([sys.executable, '-m', 'pip', 'install', 'cupy-cuda11x'], check=True)\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Installation failed: {e}\")\n",
    "        return\n",
    "\n",
    "    print(\"Installation complete.\")\n",
    "    print(\"⚠️ CRITICAL: The runtime will now RESTART automatically to load the new library.\")\n",
    "    print(\"⚠️ You may see a 'Session Crashed' or 'Kernel Restarting' message. This is NORMAL.\")\n",
    "    print(\"⚠️ AFTER the restart, please RE-RUN this cell manually.\")\n",
    "    time.sleep(2)\n",
    "    # Kill the current process to force Colab/Jupyter to restart the kernel\n",
    "    os.kill(os.getpid(), 9)\n",
    "\n",
    "try:\n",
    "    import cupy as cp\n",
    "    # aggressive check: try to allocate and execute a small kernel\n",
    "    x = cp.array([1.0, 2.0])\n",
    "    y = x * x\n",
    "    print(f\"GPU Detected: {cp.cuda.runtime.getDeviceCount()} device(s)\")\n",
    "    print(f\"CuPy Version: {cp.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"CuPy not installed.\")\n",
    "    install_compatible_cupy()\n",
    "except Exception as e:\n",
    "    # Catch CUDARuntimeError or generic exceptions related to driver mismatch\n",
    "    print(f\"GPU/CuPy check failed: {e}\")\n",
    "    if \"InsufficientDriver\" in str(e) or \"cudaErrorInsufficientDriver\" in str(e):\n",
    "        print(\"Driver is too old for the installed CuPy runtime.\")\n",
    "    install_compatible_cupy()\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cupyx.scipy.sparse as cpx\n",
    "import cupyx.scipy.sparse.csgraph as cpx_graph\n",
    "\n",
    "# Graphics settings\n",
    "plt.style.use('dark_background')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8851f53a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title 2. Data Generation & Parsing\n",
    "\n",
    "import gzip\n",
    "import io\n",
    "\n",
    "def generate_random_3sat(N, alpha, seed=None):\n",
    "    \"\"\"\n",
    "    Generates a Random 3-SAT instance.\n",
    "    N: Number of variables\n",
    "    alpha: Ratio of clauses/variables (M = alpha * N)\n",
    "    Returns: (M, 3) array of literals (1-based index, negative for NOT)\n",
    "    \"\"\"\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "    \n",
    "    M = int(N * alpha)\n",
    "    # Variables are 1..N\n",
    "    vars = np.random.randint(1, N + 1, size=(M, 3))\n",
    "    # Signs are +/- 1\n",
    "    signs = np.random.choice([-1, 1], size=(M, 3))\n",
    "    \n",
    "    clauses = vars * signs\n",
    "    return clauses, N\n",
    "\n",
    "def parse_dimacs(content_str):\n",
    "    \"\"\"Parses DIMACS CNF content string.\"\"\"\n",
    "    clauses = []\n",
    "    N = 0\n",
    "    for line in content_str.splitlines():\n",
    "        line = line.strip()\n",
    "        if not line or line.startswith('c') or line.startswith('%'): continue\n",
    "        if line.startswith('p'):\n",
    "            parts = line.split()\n",
    "            try:\n",
    "                N = int(parts[2])\n",
    "            except:\n",
    "                pass # sometimes header is malformed\n",
    "            continue\n",
    "        \n",
    "        # Parse literals\n",
    "        try:\n",
    "            lits = [int(x) for x in line.split() if x != '0']\n",
    "            if len(lits) >= 3:\n",
    "                # We take the first 3 literals for 3-SAT (truncating if >3, though ideal is proper 3-SAT)\n",
    "                # Or skip if not 3-SAT? For now, we assume input is 3-SAT.\n",
    "                # If length < 3, we might need padding.\n",
    "                # Let's strictly take triplets or skip.\n",
    "                if len(lits) == 3:\n",
    "                    clauses.append(lits)\n",
    "        except ValueError:\n",
    "            continue\n",
    "            \n",
    "    # Auto-detect N if header failed\n",
    "    clauses_np = np.array(clauses, dtype=np.int32)\n",
    "    if N == 0 and len(clauses_np) > 0:\n",
    "        N = np.max(np.abs(clauses_np))\n",
    "        \n",
    "    return clauses_np, N\n",
    "\n",
    "def download_and_parse_instance(url):\n",
    "    \"\"\"Downloads and parses a CNF instance (supports .cnf and .cnf.gz).\"\"\"\n",
    "    print(f\"Downloading {url}...\")\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "    \n",
    "    content = response.content\n",
    "    \n",
    "    # Check if gzipped\n",
    "    if url.endswith('.gz'):\n",
    "        try:\n",
    "            with gzip.open(io.BytesIO(content), 'rt') as f:\n",
    "                text_content = f.read()\n",
    "        except Exception as e:\n",
    "            print(f\"Error decompressing: {e}\")\n",
    "            return None, 0\n",
    "    else:\n",
    "        text_content = content.decode('utf-8')\n",
    "        \n",
    "    return parse_dimacs(text_content)\n",
    "\n",
    "print(\"Generators ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32dadd32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title 3. The Solver: `TetraDynamicsGPU`\n",
    "# Implements the Generalized Higher-Order Cluster Dynamics (m=2, 3, 4) + Residual Triangles.\n",
    "\n",
    "class TetraDynamicsGPU:\n",
    "    def __init__(self, clauses_np, N, omega=2.0):\n",
    "        \"\"\"\n",
    "        Initialize the Generalized Higher-Order Cluster Solver.\n",
    "        clauses_np: (M, 3) numpy array of literals (int32).\n",
    "        N: Number of variables.\n",
    "        omega: Energy scaling parameter.\n",
    "        \"\"\"\n",
    "        self.N = N\n",
    "        self.raw_clauses = clauses_np # For global energy check\n",
    "        self.omega = omega\n",
    "        \n",
    "        # --- 1. Topology Builder: Decompose Clauses into Tetrahedrons & Triangles ---\n",
    "        # We greedily find tetrahedrons (cliques of 4 vars sharing >=2 clauses).\n",
    "        print(\"Decomposing graph topology...\")\n",
    "        tetras, triangles = self._build_topology(clauses_np)\n",
    "        print(f\"Topology: {len(tetras)} Tetrahedrons, {len(triangles)} Residual Triangles.\")\n",
    "        \n",
    "        # --- 2. Prepare Data for GPU (Tetrahedrons) ---\n",
    "        # We flatten the tetrahedron list for vectorized ops.\n",
    "        # tetras structure: list of dicts {'indices': [4], 'signs': [4], 'active': [4 (bool)], 'm': int}\n",
    "        \n",
    "        self.num_tetras = len(tetras)\n",
    "        if self.num_tetras > 0:\n",
    "            t_indices = np.array([t['indices'] for t in tetras], dtype=np.int32)\n",
    "            t_signs   = np.array([t['signs'] for t in tetras], dtype=np.int8)\n",
    "            t_active  = np.array([t['active'] for t in tetras], dtype=bool)\n",
    "            t_m       = np.array([t['m'] for t in tetras], dtype=np.int8)\n",
    "            \n",
    "            self.t_indices = cp.array(t_indices)\n",
    "            self.t_signs   = cp.array(t_signs)\n",
    "            self.t_active  = cp.array(t_active) # Mask: True if node is in Active Set A\n",
    "            self.t_m       = cp.array(t_m)\n",
    "        else:\n",
    "            self.t_indices = cp.empty((0, 4), dtype=cp.int32)\n",
    "            \n",
    "        # --- 3. Prepare Data for GPU (Residual Triangles) ---\n",
    "        self.num_tris = len(triangles)\n",
    "        if self.num_tris > 0:\n",
    "            r_indices = np.array([t['indices'] for t in triangles], dtype=np.int32)\n",
    "            r_signs   = np.array([t['signs'] for t in triangles], dtype=np.int8)\n",
    "            \n",
    "            self.r_indices = cp.array(r_indices)\n",
    "            self.r_signs   = cp.array(r_signs)\n",
    "        else:\n",
    "            self.r_indices = cp.empty((0, 3), dtype=cp.int32)\n",
    "\n",
    "        # Initialize Spins\n",
    "        self.spins = cp.random.choice(cp.array([-1, 1], dtype=cp.int8), size=N + 1)\n",
    "        # We don't strictly need a dummy node for this logic, but we keep structure consistent\n",
    "        # if we ever need it. The Ghost nodes are virtual (N+1, N+2).\n",
    "        \n",
    "        # Ghost Node Indices\n",
    "        self.GHOST_PLUS = N + 1\n",
    "        self.GHOST_MINUS = N + 2\n",
    "        self.TOTAL_NODES = N + 3\n",
    "        \n",
    "        # --- 4. Precompute Probabilities ---\n",
    "        # We need thresholds for B=0, B=1, B=2 based on U and m.\n",
    "        # Store as dictionaries or small arrays? \n",
    "        # Since we vectorize, we will compute probabilities on the fly or look them up.\n",
    "        # Constants:\n",
    "        self.exp_w  = np.exp(-1.0 * omega)\n",
    "        self.exp_2w = np.exp(-2.0 * omega)\n",
    "        self.exp_3w = np.exp(-3.0 * omega)\n",
    "        self.exp_4w = np.exp(-4.0 * omega)\n",
    "\n",
    "    def _build_topology(self, clauses):\n",
    "        \"\"\"\n",
    "        Decomposes the clause list into Tetrahedrons (m=2,3,4) and Residual Triangles.\n",
    "        Greedy strategy: Prioritize higher m.\n",
    "        \"\"\"\n",
    "        # 1. Map Edges (pair of literals) to Clauses\n",
    "        # \"Edge\" here means two variables sharing a link in the factor graph.\n",
    "        # The prompt specifies: \"two triangles share an edge (two signed literals)\".\n",
    "        # So we key by (lit1, lit2) sorted.\n",
    "        \n",
    "        from collections import defaultdict\n",
    "        \n",
    "        # Each clause is identified by its index in the original list\n",
    "        # Store clauses as sets of literals\n",
    "        clause_sets = []\n",
    "        for c in clauses:\n",
    "            clause_sets.append(tuple(sorted(c)))\n",
    "            \n",
    "        active_clauses = set(range(len(clauses)))\n",
    "        \n",
    "        # Map: Edge (lit_a, lit_b) -> set of clause_indices\n",
    "        edge_map = defaultdict(set)\n",
    "        for idx, literals in enumerate(clause_sets):\n",
    "            # 3 edges per clause\n",
    "            l = literals\n",
    "            edge_map[tuple(sorted((l[0], l[1])))].add(idx)\n",
    "            edge_map[tuple(sorted((l[0], l[2])))].add(idx)\n",
    "            edge_map[tuple(sorted((l[1], l[2])))].add(idx)\n",
    "            \n",
    "        tetras = []\n",
    "        \n",
    "        # Search for Tetrahedrons\n",
    "        # Potential tetrahedrons are formed by pairs of clauses sharing an edge.\n",
    "        # We iterate edges that have >= 2 clauses.\n",
    "        \n",
    "        # To avoid duplicates, we track \"consumed\" clauses.\n",
    "        # Heuristic: Check edges with most clauses first? Or just iterate.\n",
    "        \n",
    "        # Because N might be large, we need to be efficient. \n",
    "        # We'll just iterate all edges with size >= 2.\n",
    "        \n",
    "        candidates = [] # (m, clause_indices_tuple, literals_tuple)\n",
    "        \n",
    "        checked_pairs = set()\n",
    "\n",
    "        for edge, c_indices in edge_map.items():\n",
    "            if len(c_indices) < 2:\n",
    "                continue\n",
    "            \n",
    "            # Check all pairs in this edge list\n",
    "            c_list = list(c_indices)\n",
    "            for i in range(len(c_list)):\n",
    "                for j in range(i + 1, len(c_list)):\n",
    "                    idx1, idx2 = c_list[i], c_list[j]\n",
    "                    \n",
    "                    # Form a candidate 4-set\n",
    "                    l1 = set(clause_sets[idx1])\n",
    "                    l2 = set(clause_sets[idx2])\n",
    "                    union_l = l1 | l2\n",
    "                    \n",
    "                    if len(union_l) != 4:\n",
    "                        # Should be 4 if they share exactly 2 literals (an edge)\n",
    "                        continue\n",
    "                        \n",
    "                    key = tuple(sorted(list(union_l)))\n",
    "                    if key in checked_pairs:\n",
    "                        continue\n",
    "                    checked_pairs.add(key)\n",
    "                    \n",
    "                    # Now check how many faces of this 4-set exist in 'clause_sets'\n",
    "                    # The 4-set has 4 faces (triplets).\n",
    "                    # We count how many are in our full list.\n",
    "                    \n",
    "                    # Generate 4 faces\n",
    "                    import itertools\n",
    "                    faces_found = [] # store indices\n",
    "                    \n",
    "                    # Map triplet -> original index? Slow to search list.\n",
    "                    # We can't easily search `clause_sets` if not hashed.\n",
    "                    # Let's verify presence using a set of all clauses.\n",
    "                    \n",
    "                    # Optimization: Only check this if we haven't consumed these.\n",
    "                    # But we are in the candidate gathering phase.\n",
    "                    \n",
    "                    # Count 'm'\n",
    "                    m_count = 0\n",
    "                    faces_indices = []\n",
    "                    \n",
    "                    # Check the 4 combinations\n",
    "                    u_list = sorted(list(union_l))\n",
    "                    possible_faces = list(itertools.combinations(u_list, 3))\n",
    "                    \n",
    "                    found_indices = []\n",
    "                    \n",
    "                    # We need to find the ID of these faces.\n",
    "                    # Build a lookup: triplet -> ID\n",
    "                    # Doing this once at start\n",
    "                    \n",
    "                    pass \n",
    "        \n",
    "        # --- optimized topology pass ---\n",
    "        # Re-build for speed\n",
    "        \n",
    "        clause_to_id = {c: i for i, c in enumerate(clause_sets)}\n",
    "        \n",
    "        candidates = []\n",
    "        \n",
    "        # Iterating edges again\n",
    "        seen_tetra = set()\n",
    "        \n",
    "        for edge, c_indices in edge_map.items():\n",
    "            if len(c_indices) < 2: continue\n",
    "            c_list = list(c_indices)\n",
    "            \n",
    "            for i in range(len(c_list)):\n",
    "                for j in range(i+1, len(c_list)):\n",
    "                    idx1 = c_list[i]\n",
    "                    idx2 = c_list[j]\n",
    "                    \n",
    "                    l1 = set(clause_sets[idx1])\n",
    "                    l2 = set(clause_sets[idx2])\n",
    "                    union_l = sorted(list(l1 | l2))\n",
    "                    \n",
    "                    if len(union_l) != 4: continue\n",
    "                    \n",
    "                    t_key = tuple(union_l)\n",
    "                    if t_key in seen_tetra: continue\n",
    "                    seen_tetra.add(t_key)\n",
    "                    \n",
    "                    # Check faces\n",
    "                    # We know idx1 and idx2 are present.\n",
    "                    # Check the other 2 possible faces.\n",
    "                    \n",
    "                    import itertools\n",
    "                    faces = list(itertools.combinations(union_l, 3))\n",
    "                    \n",
    "                    found_faces = []\n",
    "                    for face in faces:\n",
    "                        f_tuple = tuple(sorted(face))\n",
    "                        if f_tuple in clause_to_id:\n",
    "                            found_faces.append(clause_to_id[f_tuple])\n",
    "                            \n",
    "                    m = len(found_faces)\n",
    "                    if m >= 2:\n",
    "                        candidates.append({\n",
    "                            'm': m,\n",
    "                            'clauses': found_faces,\n",
    "                            'literals': union_l\n",
    "                        })\n",
    "                        \n",
    "        # Sort candidates by m descending\n",
    "        candidates.sort(key=lambda x: x['m'], reverse=True)\n",
    "        \n",
    "        # Greedy Assignment\n",
    "        final_tetras = []\n",
    "        consumed_mask = np.zeros(len(clauses), dtype=bool)\n",
    "        \n",
    "        for cand in candidates:\n",
    "            # Check if any clause is already consumed\n",
    "            if any(consumed_mask[idx] for idx in cand['clauses']):\n",
    "                continue\n",
    "                \n",
    "            # Consume\n",
    "            for idx in cand['clauses']:\n",
    "                consumed_mask[idx] = True\n",
    "                \n",
    "            # Build Tetra Object\n",
    "            # Need to define 'Active' mask.\n",
    "            # \"Active\" node is one opposite to an Active Face.\n",
    "            # Active Face = a clause that exists (is in cand['clauses']).\n",
    "            \n",
    "            # Map literals to 0..3 local indices\n",
    "            lits = cand['literals'] # the 4 literals (signed)\n",
    "            \n",
    "            # Store indices (abs(lit)-1) and signs\n",
    "            indices = [abs(x)-1 for x in lits]\n",
    "            signs = [int(np.sign(x)) for x in lits]\n",
    "            \n",
    "            active_mask = [False] * 4\n",
    "            \n",
    "            # Check each vertex v. If face opposite to v is in 'cand['clauses']', v is active.\n",
    "            # Face opposite to local index i is the triplet excluding i.\n",
    "            import itertools\n",
    "            for i in range(4):\n",
    "                # Form face excluding i\n",
    "                face_lits = [lits[k] for k in range(4) if k != i]\n",
    "                face_tuple = tuple(sorted(face_lits))\n",
    "                \n",
    "                # Is this face in our consumed list?\n",
    "                # We need to check if face_tuple corresponds to one of the consumed IDs\n",
    "                # Using lookup\n",
    "                if face_tuple in clause_to_id:\n",
    "                    if clause_to_id[face_tuple] in cand['clauses']:\n",
    "                        active_mask[i] = True\n",
    "            \n",
    "            final_tetras.append({\n",
    "                'indices': indices,\n",
    "                'signs': signs,\n",
    "                'active': active_mask,\n",
    "                'm': cand['m']\n",
    "            })\n",
    "            \n",
    "        # Collect Residuals\n",
    "        final_tris = []\n",
    "        for i in range(len(clauses)):\n",
    "            if not consumed_mask[i]:\n",
    "                c = clause_sets[i]\n",
    "                final_tris.append({\n",
    "                    'indices': [abs(x)-1 for x in c],\n",
    "                    'signs': [int(np.sign(x)) for x in c]\n",
    "                })\n",
    "                \n",
    "        return final_tetras, final_tris\n",
    "\n",
    "    def step(self):\n",
    "        \"\"\"\n",
    "        Swendsen-Wang Step with Generalized Dynamics.\n",
    "        \"\"\"\n",
    "        # --- 1. Tetrahedrons Dynamics ---\n",
    "        if self.num_tetras > 0:\n",
    "            # Gather spins (T, 4)\n",
    "            t_spins = self.spins[self.t_indices]\n",
    "            # Check sat (T, 4)\n",
    "            is_sat = (t_spins == self.t_signs)\n",
    "            # k: num sat (T,)\n",
    "            k = cp.sum(is_sat, axis=1)\n",
    "            \n",
    "            # Determine Energy State U\n",
    "            # Conditions:\n",
    "            # U = 0 if (k >= 2) OR (k==1 AND sat_node is Inactive)\n",
    "            # U = w if (k == 1 AND sat_node is Active)\n",
    "            # U = m*w if (k == 0)\n",
    "            \n",
    "            # Identify if the single satisfied node is Active (for k=1 case)\n",
    "            # sat_and_active = is_sat & self.t_active\n",
    "            # is_sat_active_any = cp.any(sat_and_active, axis=1)\n",
    "            \n",
    "            # We can compute U directly or implicit probabilities.\n",
    "            # Let's map to a State Index S:\n",
    "            # 0: Energy mw (k=0)\n",
    "            # 1: Energy w  (k=1, Active)\n",
    "            # 2: Energy 0  (k>=2 or k=1 Inactive)\n",
    "            \n",
    "            state = cp.zeros(self.num_tetras, dtype=cp.int8) # Default 0 (k=0)\n",
    "            \n",
    "            # Mask k=1\n",
    "            mask_k1 = (k == 1)\n",
    "            # Check if the sat node is Active\n",
    "            # For k=1, exactly one is_sat is True. is_sat & t_active gives True if that one is active.\n",
    "            mask_k1_active = mask_k1 & cp.any(is_sat & self.t_active, axis=1)\n",
    "            mask_k1_inactive = mask_k1 & (~mask_k1_active)\n",
    "            \n",
    "            mask_k_ge_2 = (k >= 2)\n",
    "            \n",
    "            # Assign states\n",
    "            # State 0 is default\n",
    "            state[mask_k1_active] = 1\n",
    "            state[mask_k1_inactive] = 2\n",
    "            state[mask_k_ge_2] = 2\n",
    "            \n",
    "            # Generate Bond B (0, 1, 2)\n",
    "            # We need vectorized random sampling based on State and m.\n",
    "            # Probabilities depend on m.\n",
    "            # We can use a unified random number u.\n",
    "            \n",
    "            u = cp.random.random(self.num_tetras, dtype=cp.float32)\n",
    "            bonds = cp.zeros(self.num_tetras, dtype=cp.int8)\n",
    "            \n",
    "            # We must apply rules for m=2,3,4.\n",
    "            # To vectorize efficiently, we can lookup thresholds based on (m, state).\n",
    "            # Or handle each m separately. Separating by m is clearer.\n",
    "            \n",
    "            for m_val in [2, 3, 4]:\n",
    "                mask_m = (self.t_m == m_val)\n",
    "                if not cp.any(mask_m): continue\n",
    "                \n",
    "                # Sub-masks\n",
    "                mask_S0 = mask_m & (state == 0) # U = mw\n",
    "                mask_S1 = mask_m & (state == 1) # U = w\n",
    "                mask_S2 = mask_m & (state == 2) # U = 0\n",
    "                \n",
    "                # --- State 0 (k=0) ---\n",
    "                # P(B=0)=1. Always 0.\n",
    "                \n",
    "                # --- State 1 (k=1 Active) ---\n",
    "                # Energy w.\n",
    "                # P(B=0) = e^{-(m-1)w}\n",
    "                # P(B=1) = 1 - P(B=0)\n",
    "                # P(B=2) = 0\n",
    "                p_b0_s1 = cp.exp(-(m_val - 1) * self.omega)\n",
    "                \n",
    "                mask_S1_B1 = mask_S1 & (u >= p_b0_s1)\n",
    "                bonds[mask_S1_B1] = 1\n",
    "                \n",
    "                # --- State 2 (Energy 0) ---\n",
    "                # P(B=0) = e^{-mw}\n",
    "                # P(B=1) = e^{-w} - e^{-mw}\n",
    "                # P(B=2) = 1 - e^{-w}\n",
    "                \n",
    "                p_b0_s2 = cp.exp(-m_val * self.omega)\n",
    "                p_b1_cum_s2 = cp.exp(-1.0 * self.omega) # P(B<=1) = e^{-w}\n",
    "                \n",
    "                # B=1 if p_b0 <= u < p_b1_cum\n",
    "                mask_S2_B1 = mask_S2 & (u >= p_b0_s2) & (u < p_b1_cum_s2)\n",
    "                bonds[mask_S2_B1] = 1\n",
    "                \n",
    "                # B=2 if u >= p_b1_cum\n",
    "                mask_S2_B2 = mask_S2 & (u >= p_b1_cum_s2)\n",
    "                bonds[mask_S2_B2] = 2\n",
    "            \n",
    "            # --- Witness Selection (Vectorized) ---\n",
    "            # Generate random priorities for all nodes in tetrahedrons\n",
    "            priorities = cp.random.random((self.num_tetras, 4), dtype=cp.float32)\n",
    "            \n",
    "            # Arrays to store witnesses (Global Indices)\n",
    "            # We can have up to 2 witnesses per tetra.\n",
    "            # Initialize with -1 (no witness)\n",
    "            w1_global = cp.full(self.num_tetras, -1, dtype=cp.int32)\n",
    "            w2_global = cp.full(self.num_tetras, -1, dtype=cp.int32)\n",
    "            w1_sign   = cp.zeros(self.num_tetras, dtype=cp.int8)\n",
    "            w2_sign   = cp.zeros(self.num_tetras, dtype=cp.int8)\n",
    "            \n",
    "            # --- Case B=1: Freeze 1 Satisfied Node ---\n",
    "            mask_active_1 = (bonds >= 1) # B=1 or B=2 both need at least 1 witness\n",
    "            \n",
    "            # Priority Logic: \"1er sommet satisfait dans l'ordre pi\"\n",
    "            # Mask priorities of unsatisfied nodes to -1\n",
    "            p_sat = priorities.copy()\n",
    "            p_sat[~is_sat] = -2.0 # Unsatisfied\n",
    "            \n",
    "            # Select Max Priority\n",
    "            idx_w1 = cp.argmax(p_sat, axis=1) # Local index 0..3\n",
    "            \n",
    "            # Store w1 for Active bonds\n",
    "            w1_global = cp.where(mask_active_1, \n",
    "                                 cp.take_along_axis(self.t_indices, idx_w1[:, None], axis=1).flatten(),\n",
    "                                 w1_global)\n",
    "            w1_sign = cp.where(mask_active_1,\n",
    "                               cp.take_along_axis(self.t_signs, idx_w1[:, None], axis=1).flatten(),\n",
    "                               w1_sign)\n",
    "            \n",
    "            # --- Case B=2: Freeze 2nd Node OR Inactive Node ---\n",
    "            mask_B2 = (bonds == 2)\n",
    "            \n",
    "            # Logic:\n",
    "            # if exists sat node in Inactive (I): Freeze 1 such node (already done in B=1 step? No.)\n",
    "            # Wait, the rule for B=2 says:\n",
    "            # \"si existe +1 sur I: geler 1 inactif (+1). Sinon: geler 2 sommets (+1).\"\n",
    "            \n",
    "            # So, for B=2, we must check if we picked an Inactive node as w1?\n",
    "            # Or does \"1er inactif (+1)\" imply we prioritize Inactive?\n",
    "            # \"si existe +1 sur I: geler 1 tel sommet (le premier dans pi parmi ceux-là).\"\n",
    "            # This implies a priority filter: Filter for (Sat AND Inactive). If not empty, pick max.\n",
    "            # Else (if only Active sat), pick top 2 Sat.\n",
    "            \n",
    "            # We need to refine w1 selection for B=2 specifically.\n",
    "            # Let's re-calculate w1, w2 for B=2 rows.\n",
    "            \n",
    "            if cp.any(mask_B2):\n",
    "                # Subset of data for B=2\n",
    "                # This is tricky to do in-place vectorized without masking.\n",
    "                # Let's just adjust the priorities for B=2 case before argmax?\n",
    "                # No, B=1 and B=2 have different selection rules for w1.\n",
    "                \n",
    "                # Correction:\n",
    "                # B=1: Any Sat.\n",
    "                # B=2: Priority to Inactive Sat.\n",
    "                \n",
    "                # Let's compute specific targets for B=2\n",
    "                \n",
    "                # Check existence of Sat+Inactive\n",
    "                has_sat_inactive = cp.any(is_sat & (~self.t_active), axis=1)\n",
    "                \n",
    "                # Sub-mask for B=2\n",
    "                mask_B2_Inactive = mask_B2 & has_sat_inactive\n",
    "                mask_B2_ActiveOnly = mask_B2 & (~has_sat_inactive)\n",
    "                \n",
    "                # For mask_B2_Inactive: Pick max priority among (Sat & Inactive)\n",
    "                # We can modify p_sat temporary for these rows?\n",
    "                # Better: construct specific mask.\n",
    "                \n",
    "                # 1. Update w1 for B=2 & Inactive\n",
    "                # mask out Active nodes in priority\n",
    "                p_sat_inactive = p_sat.copy()\n",
    "                p_sat_inactive[self.t_active] = -2.0 # Mask active\n",
    "                \n",
    "                idx_w1_inactive = cp.argmax(p_sat_inactive, axis=1)\n",
    "                \n",
    "                # Apply update\n",
    "                w1_global = cp.where(mask_B2_Inactive, \n",
    "                                     cp.take_along_axis(self.t_indices, idx_w1_inactive[:,None], axis=1).flatten(),\n",
    "                                     w1_global)\n",
    "                w1_sign = cp.where(mask_B2_Inactive,\n",
    "                                   cp.take_along_axis(self.t_signs, idx_w1_inactive[:,None], axis=1).flatten(),\n",
    "                                   w1_sign)\n",
    "                                   \n",
    "                # w2 remains -1 for this case (only 1 witness needed)\n",
    "                \n",
    "                # 2. Update w1, w2 for B=2 & Active Only (Need 2 witnesses)\n",
    "                # w1 is already picked correctly (Max of Sat) because only Active are Sat.\n",
    "                # We just need w2 (2nd Max of Sat).\n",
    "                \n",
    "                # Mask the chosen w1\n",
    "                p_sat_w2 = p_sat.copy()\n",
    "                rows = cp.arange(self.num_tetras)\n",
    "                # We need the local index of the current w1 to mask it.\n",
    "                # Recover local index?\n",
    "                # argmax was used on p_sat.\n",
    "                idx_w1_current = cp.argmax(p_sat, axis=1)\n",
    "                \n",
    "                p_sat_w2[rows, idx_w1_current] = -3.0\n",
    "                \n",
    "                idx_w2 = cp.argmax(p_sat_w2, axis=1)\n",
    "                \n",
    "                # Apply\n",
    "                w2_global = cp.where(mask_B2_ActiveOnly,\n",
    "                                     cp.take_along_axis(self.t_indices, idx_w2[:,None], axis=1).flatten(),\n",
    "                                     w2_global)\n",
    "                w2_sign = cp.where(mask_B2_ActiveOnly,\n",
    "                                   cp.take_along_axis(self.t_signs, idx_w2[:,None], axis=1).flatten(),\n",
    "                                   w2_sign)\n",
    "\n",
    "            # Store edges for graph\n",
    "            # Active B >= 1\n",
    "            mask_has_w1 = (bonds >= 1)\n",
    "            mask_has_w2 = (w2_global != -1)\n",
    "            \n",
    "            src_t1 = w1_global[mask_has_w1]\n",
    "            tgt_t1 = cp.where(w1_sign[mask_has_w1] > 0, self.GHOST_PLUS, self.GHOST_MINUS)\n",
    "            \n",
    "            src_t2 = w2_global[mask_has_w2]\n",
    "            tgt_t2 = cp.where(w2_sign[mask_has_w2] > 0, self.GHOST_PLUS, self.GHOST_MINUS)\n",
    "        \n",
    "        else:\n",
    "            # Empty placeholders\n",
    "            src_t1, tgt_t1 = cp.array([], dtype=cp.int32), cp.array([], dtype=cp.int32)\n",
    "            src_t2, tgt_t2 = cp.array([], dtype=cp.int32), cp.array([], dtype=cp.int32)\n",
    "\n",
    "\n",
    "        # --- 2. Residual Triangles Dynamics ---\n",
    "        if self.num_tris > 0:\n",
    "            # Gather spins\n",
    "            r_spins = self.spins[self.r_indices]\n",
    "            # Sat\n",
    "            r_is_sat = (r_spins == self.r_signs)\n",
    "            r_clause_sat = cp.any(r_is_sat, axis=1)\n",
    "            \n",
    "            # Sampling: Only if satisfied\n",
    "            # P(Freeze) = 1 - e^-w\n",
    "            p_freeze = 1.0 - self.exp_w\n",
    "            u_r = cp.random.random(self.num_tris, dtype=cp.float32)\n",
    "            \n",
    "            mask_freeze = r_clause_sat & (u_r < p_freeze)\n",
    "            \n",
    "            # Select 1 witness (Random Sat)\n",
    "            r_priorities = cp.random.random((self.num_tris, 3), dtype=cp.float32)\n",
    "            r_priorities[~r_is_sat] = -1.0\n",
    "            \n",
    "            idx_r_w1 = cp.argmax(r_priorities, axis=1)\n",
    "            \n",
    "            # Extract\n",
    "            src_r = cp.take_along_axis(self.r_indices, idx_r_w1[:,None], axis=1).flatten()[mask_freeze]\n",
    "            sign_r = cp.take_along_axis(self.r_signs, idx_r_w1[:,None], axis=1).flatten()[mask_freeze]\n",
    "            tgt_r = cp.where(sign_r > 0, self.GHOST_PLUS, self.GHOST_MINUS)\n",
    "            \n",
    "        else:\n",
    "            src_r, tgt_r = cp.array([], dtype=cp.int32), cp.array([], dtype=cp.int32)\n",
    "            \n",
    "        # --- 3. Graph Construction & Cluster Flip ---\n",
    "        all_src = cp.concatenate([src_t1, src_t2, src_r])\n",
    "        all_tgt = cp.concatenate([tgt_t1, tgt_t2, tgt_r])\n",
    "        \n",
    "        if len(all_src) > 0:\n",
    "            weights = cp.ones(len(all_src), dtype=cp.float32)\n",
    "            # Create CSR Matrix\n",
    "            adj = cpx.coo_matrix((weights, (all_src, all_tgt)), shape=(self.TOTAL_NODES, self.TOTAL_NODES), dtype=cp.float32)\n",
    "            adj = adj.tocsr()\n",
    "            adj = adj + adj.T\n",
    "            \n",
    "            # Components\n",
    "            n_components, labels = cpx_graph.connected_components(adj, directed=False)\n",
    "            \n",
    "            # Determine Ghost Labels\n",
    "            l_plus = labels[self.GHOST_PLUS]\n",
    "            l_minus = labels[self.GHOST_MINUS]\n",
    "            \n",
    "            # Random Flips\n",
    "            comp_flips = cp.random.choice(cp.array([-1, 1], dtype=cp.int8), size=n_components)\n",
    "            \n",
    "            new_spins = comp_flips[labels[:self.N+1]]\n",
    "            \n",
    "            # Fix Ghosts\n",
    "            mask_plus = (labels[:self.N+1] == l_plus)\n",
    "            mask_minus = (labels[:self.N+1] == l_minus)\n",
    "            \n",
    "            new_spins[mask_plus] = 1\n",
    "            new_spins[mask_minus] = -1\n",
    "            \n",
    "            # Conflict handling (Rare)\n",
    "            mask_conflict = mask_plus & mask_minus\n",
    "            if cp.any(mask_conflict):\n",
    "                new_spins[mask_conflict] = self.spins[mask_conflict] # Freeze to old\n",
    "                \n",
    "            self.spins = new_spins\n",
    "        else:\n",
    "            # No bonds? Just flip everything randomly? \n",
    "            # Standard SW: If no edges, every node is a cluster.\n",
    "            # Here we haven't built the full graph of N nodes, only witnessing edges.\n",
    "            # Implicitly, nodes not in 'all_src' are singletons.\n",
    "            # They should flip 50/50.\n",
    "            # The code above relies on 'labels' covering all nodes.\n",
    "            # If adj is empty, connected_components returns N components (if shape is correct).\n",
    "            # But coo_matrix shape is (TOTAL, TOTAL), so it should work.\n",
    "            \n",
    "            # However, empty COO might be tricky.\n",
    "            pass\n",
    "            \n",
    "            # Fallback for empty edges (Global random flip)\n",
    "            self.spins = cp.random.choice(cp.array([-1, 1], dtype=cp.int8), size=self.N + 1)\n",
    "\n",
    "    def energy(self):\n",
    "        \"\"\"Global 3-SAT Energy (Fraction Unsatisfied Clauses).\"\"\"\n",
    "        # (M, 3)\n",
    "        indices = cp.array(self.raw_clauses - 1) # 0-based\n",
    "        signs = cp.array(np.sign(self.raw_clauses), dtype=cp.int8)\n",
    "        indices = cp.abs(indices)\n",
    "        \n",
    "        current_spins = self.spins[indices]\n",
    "        is_sat = (current_spins == signs)\n",
    "        clause_sat = cp.any(is_sat, axis=1)\n",
    "        \n",
    "        return 1.0 - cp.mean(clause_sat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf83c3af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title 4. Baseline: `MetropolisGPU`\n",
    "# A simple parallel Metropolis sampler for comparison.\n",
    "\n",
    "class MetropolisGPU:\n",
    "    def __init__(self, clauses_np, N, beta=2.0):\n",
    "        self.N = N\n",
    "        self.indices = cp.array(np.abs(clauses_np) - 1, dtype=cp.int32)\n",
    "        self.signs = cp.array(np.sign(clauses_np), dtype=cp.int8)\n",
    "        self.spins = cp.random.choice(cp.array([-1, 1], dtype=cp.int8), size=N)\n",
    "        self.beta = beta\n",
    "        \n",
    "    def step(self):\n",
    "        # Propose random flips (vectorized batch flip often bad for dense, \n",
    "        # but for sparse SAT, we can try flipping a fraction or just 1. \n",
    "        # For fairness, let's flip N variables in parallel with acceptance).\n",
    "        \n",
    "        # 1. Compute current energy (unsat count) per clause\n",
    "        # This is expensive to do fully incrementally on python level,\n",
    "        # so we do a naive full recalculation or a partial optimized one.\n",
    "        # For speed in this demo, we'll just do a 1% spin flip batch.\n",
    "        \n",
    "        n_flip = max(1, int(self.N * 0.01))\n",
    "        flip_indices = cp.random.randint(0, self.N, size=n_flip)\n",
    "        \n",
    "        # Calc global energy before\n",
    "        e_old = self.get_energy_count()\n",
    "        \n",
    "        # Flip\n",
    "        self.spins[flip_indices] *= -1\n",
    "        \n",
    "        # Calc global energy after\n",
    "        e_new = self.get_energy_count()\n",
    "        \n",
    "        # Metropolis acceptance\n",
    "        delta_E = e_new - e_old\n",
    "        if delta_E > 0:\n",
    "            p = cp.exp(-self.beta * delta_E)\n",
    "            if cp.random.random() > p:\n",
    "                # Reject: Flip back\n",
    "                self.spins[flip_indices] *= -1\n",
    "\n",
    "    def get_energy_count(self):\n",
    "        current = self.spins[self.indices]\n",
    "        is_sat = (current == self.signs)\n",
    "        clause_sat = cp.any(is_sat, axis=1)\n",
    "        return cp.sum(~clause_sat)\n",
    "\n",
    "    def energy(self):\n",
    "        return self.get_energy_count() / len(self.indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9586d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title 5. Execution & Benchmarking\n",
    "\n",
    "# Configuration\n",
    "SOURCE = \"SATLIB (uf250)\" # @param [\"Random\", \"SATLIB (uf250)\", \"Custom URL\"]\n",
    "CUSTOM_URL = \"\" # @param {type:\"string\"}\n",
    "\n",
    "# Random Params\n",
    "N = 2000          # Number of variables (for Random)\n",
    "alpha = 4.2       # Clause density (for Random)\n",
    "\n",
    "# Solver Params\n",
    "steps = 500       # Simulation steps\n",
    "omega = 3.5       # Interaction strength (Tetra)\n",
    "beta_base = 4.0   # Inv Temp (Metropolis)\n",
    "compare_baseline = True # @param {type:\"boolean\"} \n",
    "\n",
    "# Load Data\n",
    "if SOURCE == \"Random\":\n",
    "    print(f\"Generating Random 3-SAT: N={N}, M={int(alpha*N)}...\")\n",
    "    clauses, real_N = generate_random_3sat(N, alpha, seed=42)\n",
    "elif SOURCE == \"SATLIB (uf250)\":\n",
    "    # Example hard instance from SATLIB\n",
    "    url = \"https://www.cs.ubc.ca/~hoos/SATLIB/Benchmarks/SAT/RND3SAT/uf250-1065/uf250-01.cnf\"\n",
    "    print(f\"Fetching {url}...\")\n",
    "    clauses, real_N = download_and_parse_instance(url)\n",
    "else:\n",
    "    if not CUSTOM_URL:\n",
    "        print(\"Error: Please provide a Custom URL.\")\n",
    "        clauses, real_N = np.array([]), 0\n",
    "    else:\n",
    "        clauses, real_N = download_and_parse_instance(CUSTOM_URL)\n",
    "\n",
    "if len(clauses) == 0:\n",
    "    print(\"No valid clauses found. Exiting.\")\n",
    "else:\n",
    "    print(f\"Loaded Instance: N={real_N}, M={len(clauses)}\")\n",
    "\n",
    "    # --- Run Tetra Dynamics ---\n",
    "    print(\"Initializing TetraDynamicsGPU...\")\n",
    "    tetra_solver = TetraDynamicsGPU(clauses, real_N, omega=omega)\n",
    "\n",
    "    tetra_energies = []\n",
    "    start_t = time.time()\n",
    "    for i in range(steps):\n",
    "        tetra_solver.step()\n",
    "        if i % 10 == 0:\n",
    "            e = tetra_solver.energy().item()\n",
    "            tetra_energies.append(e)\n",
    "            # print(f\"Step {i}: E={e:.4f}\")\n",
    "    end_t = time.time()\n",
    "    print(f\"Tetra Dynamics Time: {end_t - start_t:.2f}s\")\n",
    "\n",
    "    # --- Run Baseline (Optional) ---\n",
    "    metro_energies = []\n",
    "    if compare_baseline:\n",
    "        print(\"Initializing MetropolisGPU...\")\n",
    "        metro_solver = MetropolisGPU(clauses, real_N, beta=beta_base)\n",
    "        \n",
    "        start_t = time.time()\n",
    "        for i in range(steps):\n",
    "            metro_solver.step()\n",
    "            if i % 10 == 0:\n",
    "                e = metro_solver.energy().item()\n",
    "                metro_energies.append(e)\n",
    "        end_t = time.time()\n",
    "        print(f\"Metropolis Time: {end_t - start_t:.2f}s\")\n",
    "\n",
    "    # --- Plotting ---\n",
    "    x_axis = np.arange(0, steps, 10)\n",
    "    plt.figure()\n",
    "    plt.plot(x_axis, tetra_energies, label='Tetra Cluster Dynamics (Ours)', color='cyan', linewidth=2)\n",
    "    if compare_baseline:\n",
    "        plt.plot(x_axis, metro_energies, label='Standard Metropolis', color='orange', alpha=0.7)\n",
    "\n",
    "    plt.xlabel('MC Steps')\n",
    "    plt.ylabel('Fraction Unsatisfied (Energy)')\n",
    "    plt.title(rf'3-SAT Optimization: N={real_N}, M={len(clauses)}')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.2)\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
